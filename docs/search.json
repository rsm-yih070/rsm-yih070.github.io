[
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Projects",
    "section": "",
    "text": "A Replication of Karlan and List (2007)\n\n\n\n\n\n\nYiwei(Jerry) Huang\n\n\nMay 26, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nYiwei(Jerry) Huang\n\n\nMay 26, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMultinomial Logit Model\n\n\n\n\n\n\nYiwei(Jerry) Huang\n\n\nMay 26, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Yiwei(Jerry) Huang",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "hw1_questions.html",
    "href": "hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "hw1_questions.html#introduction",
    "href": "hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "hw1_questions.html#data",
    "href": "hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\ntodo: Read the data into R/Python and describe the data\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pyrsm as rsm\ndf = pd.read_stata(r\"c:\\Users\\13816\\Desktop\\karlan_list_2007.dta\")\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\ntodo: test a few variables other than the key outcome variables (for example, test months since last donation) to see if the treatment and control groups are statistically significantly different at the 95% confidence level. Do each as a t-test and separately as a linear regression, and confirm you get the exact same results from both methods. When doing a t-test, use the formula in the class slides. When doing the linear regression, regress for example mrm2 on treatment and look at the estimated coefficient on the treatment variable. It might be helpful to compare parts of your analysis to Table 1 in the paper. Be sure to comment on your results (hint: why is Table 1 included in the paper).\n\ntreat = df[df['treatment'] == 1]['mrm2']\ncontrol = df[df['treatment'] == 0]['mrm2']\n\nmean_treat = treat.mean()\nmean_control = control.mean()\nvar_treat = treat.var(ddof=1)\nvar_control = control.var(ddof=1)\nn_treat = treat.count()\nn_control = control.count()\n\nnumerator = mean_treat - mean_control\ndenominator = np.sqrt(var_treat / n_treat + var_control / n_control)\nt_stat = numerator / denominator\n\nprint(\"Mean Treatment:\", round(mean_treat,3))\nprint(\"Std Treatment:\", round(np.sqrt(var_treat),3))\n\nprint(\"Mean Control:\", round(mean_control,3))\nprint(\"Std Control:\", round(np.sqrt(var_control),3))\nprint(\"Overall Mean:\", round(df['mrm2'].mean(),3))\nprint(\"Overall Std:\", round(df['mrm2'].std(),3))\nprint(\"t test:\", t_stat)\n\nMean Treatment: 13.012\nStd Treatment: 12.086\nMean Control: 12.998\nStd Control: 12.074\nOverall Mean: 13.007\nOverall Std: 12.081\nt test: 0.1195315522817725\n\n\n\nmodel = rsm.model.regress(data = df, rvar = 'mrm2', evar = 'treatment')\nmodel.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : mrm2\nExplanatory variables: treatment\nNull hyp.: the effect of x on mrm2 is zero\nAlt. hyp.: the effect of x on mrm2 is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept       12.998      0.094  138.979  &lt; .001  ***\ntreatment        0.014      0.115    0.119   0.905     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.0\nF-statistic: 0.014 df(1, 50080), p.value 0.905\nNr obs: 50,082 (1 obs. dropped)\n\n\nThe manual t-test and linear regression results for mrm2 (months since last donation) show no statistically significant difference between the treatment and control groups (t ≈ 0.12, p ≈ 0.905), indicating that this key pre-treatment variable is well balanced. This supports the validity of the random assignment and confirms that the experimental groups are comparable before the intervention. Table 1 is included in the paper to show that randomization produced balanced groups across a wide range of covariates, ensuring that any observed differences in outcomes can be attributed to the treatment rather than pre-existing differences."
  },
  {
    "objectID": "hw1_questions.html#experimental-results",
    "href": "hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\ntodo: make a barplot with two bars. Each bar is the proportion of people who donated. One bar for treatment and one bar for control.\n\nprop_donated = df.groupby('treatment')['gave'].mean().reset_index()\nprop_donated['group'] = prop_donated['treatment'].map({0: 'Control', 1: 'Treatment'})\n\nsns.barplot(data=prop_donated, x='group', y='gave')\nplt.ylabel(\"Proportion Who Donated\")\nplt.title(\"Donation Rate by Group\")\nplt.show()\n\n\n\n\n\n\n\n\ntodo: run a t-test between the treatment and control groups on the binary outcome of whether any charitable donation was made. Also run a bivariate linear regression that demonstrates the same finding. (It may help to confirm your calculations match Table 2a Panel A.) Report your statistical results and interpret them in the context of the experiment (e.g., if you found a difference with a small p-value or that was statistically significant at some threshold, what have you learned about human behavior? Use mostly English words, not numbers or stats, to explain your finding.)\n\ntreat = df[df['treatment'] == 1]['gave']\ncontrol = df[df['treatment'] == 0]['gave']\n\nmean_diff = treat.mean() - control.mean()\nvar_treat = treat.var()\nvar_control = control.var()\nn_treat = len(treat)\nn_control = len(control)\n\nt_stat = mean_diff / np.sqrt(var_treat / n_treat + var_control / n_control)\nprint(\"T-statistic:\", t_stat)\n\nT-statistic: 3.2094621908279835\n\n\n\nmodel = rsm.model.regress(data = df, rvar = 'gave', evar = 'treatment')\nmodel.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : gave\nExplanatory variables: treatment\nNull hyp.: the effect of x on gave is zero\nAlt. hyp.: the effect of x on gave is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.018      0.001   16.225  &lt; .001  ***\ntreatment        0.004      0.001    3.101   0.002   **\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: 0.0\nF-statistic: 9.618 df(1, 50081), p.value 0.002\nNr obs: 50,083\n\n\nThe statistical analysis shows that individuals who received the treatment — a matching donation offer — were significantly more likely to make a charitable contribution compared to those who received the control letter. Both the t-test and linear regression indicate that this difference is unlikely to be due to chance. This suggests that the presence of a matching grant acts as a powerful motivator, nudging people toward taking action. In the context of human behavior, this reveals that even a subtle change in framing — like knowing one’s gift will be matched — can meaningfully increase the likelihood of giving. It highlights how social cues or perceived amplification of impact can influence decision-making in charitable contexts.\ntodo: run a probit regression where the outcome variable is whether any charitable donation was made and the explanatory variable is assignment to treatment or control. Confirm that your results replicate Table 3 column 1 in the paper.\n\nfrom statsmodels.discrete.discrete_model import Probit\n\nprobit_model = Probit.from_formula('gave ~ treatment', data=df)\nprobit_result = probit_model.fit()\nprint(probit_result.summary())\nmfx = probit_result.get_margeff()\nprint(mfx.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                   gave   No. Observations:                50083\nModel:                         Probit   Df Residuals:                    50081\nMethod:                           MLE   Df Model:                            1\nDate:                Sun, 20 Apr 2025   Pseudo R-squ.:               0.0009783\nTime:                        15:17:21   Log-Likelihood:                -5030.5\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                  0.001696\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -2.1001      0.023    -90.073      0.000      -2.146      -2.054\ntreatment      0.0868      0.028      3.113      0.002       0.032       0.141\n==============================================================================\n       Probit Marginal Effects       \n=====================================\nDep. Variable:                   gave\nMethod:                          dydx\nAt:                           overall\n==============================================================================\n                dy/dx    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\ntreatment      0.0043      0.001      3.104      0.002       0.002       0.007\n==============================================================================\n\n\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\ntodo: Use a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not. For example, does the 2:1 match rate lead increase the likelihood that someone donates as compared to the 1:1 match rate? Do your results support the “figures suggest” comment the authors make on page 8?\n\n# t-test between 2:1 and 1:1 ratios\ngave_1_1 = df[df['ratio'] == 1]['gave']\ngave_2_1 = df[df['ratio'] == 2]['gave']\n\nfrom scipy.stats import ttest_ind\nt_stat, p_val = ttest_ind(gave_2_1, gave_1_1, equal_var=False)\n\nprint(\"2:1 vs 1:1 Match Rate:\")\nprint(f\"T-stat: {t_stat:.3f}, P-value: {p_val:.3f}\")\n\n2:1 vs 1:1 Match Rate:\nT-stat: 0.965, P-value: 0.335\n\n\n\n# t-test between 3:1 and 1:1 ratios\ngave_3_1 = df[df['ratio'] == 3]['gave']\ngave_1_1 = df[df['ratio'] == 1]['gave']\n\nt_stat, p_val = ttest_ind(gave_3_1, gave_1_1, equal_var=False)\nprint(\"3:1 vs 1:1 Match Rate:\")\nprint(f\"T-stat: {t_stat:.3f}, P-value: {p_val:.3f}\")\n\n3:1 vs 1:1 Match Rate:\nT-stat: 1.015, P-value: 0.310\n\n\n\n# t-test between 3:1 and 2:1 ratios\ngave_3_1 = df[df['ratio'] == 3]['gave']\ngave_2_1 = df[df['ratio'] == 2]['gave']\n\nt_stat, p_val = ttest_ind(gave_3_1, gave_2_1, equal_var=False)\nprint(\"3:1 vs 2:1 Match Rate:\")\nprint(f\"T-stat: {t_stat:.3f}, P-value: {p_val:.3f}\")\n\n3:1 vs 2:1 Match Rate:\nT-stat: 0.050, P-value: 0.960\n\n\nMy results clearly support the authors’ conclusion: while higher match ratios may show slightly higher donation rates numerically, those differences are not statistically significant, and therefore do not provide strong evidence that larger match sizes are more effective than 1:1 matches.\ntodo: Assess the same issue using a regression. Specifically, create the variable ratio1 then regress gave on ratio1, ratio2, and ratio3 (or alternatively, regress gave on the categorical variable ratio). Interpret the coefficients and their statistical precision.\n\nmodel = rsm.model.regress(data = df, rvar = 'gave', evar = 'ratio')\nmodel.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : gave\nExplanatory variables: ratio\nNull hyp.: the effect of x on gave is zero\nAlt. hyp.: the effect of x on gave is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.018      0.001   16.225  &lt; .001  ***\nratio[1]         0.003      0.002    1.661   0.097    .\nratio[2]         0.005      0.002    2.744   0.006   **\nratio[3]         0.005      0.002    2.802   0.005   **\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: 0.0\nF-statistic: 3.665 df(3, 50079), p.value 0.012\nNr obs: 50,083\n\n\nRegression results show that all three match ratios (1:1, 2:1, 3:1) are associated with slightly higher donation rates compared to the control group, but the differences are small. The coefficient for ratio[1] (1:1 match) is 0.003 and marginally significant (p = 0.097), while the coefficients for ratio[2] (2:1) and ratio[3] (3:1) are both 0.005 and statistically significant at the 1% level (p = 0.006 and p = 0.005). However, the differences between them are not large in magnitude — all are within 0.002 of each other — suggesting that although offering any match tends to increase donations, there is little evidence that larger match sizes lead to proportionally greater increases. This supports the conclusion that the presence of a match matters more than its generosity, and that people are generally responsive to the signal of support, not necessarily the size of the match.\ntodo: Calculate the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios. Do this directly from the data, and do it by computing the differences in the fitted coefficients of the previous regression. what do you conclude regarding the effectiveness of different sizes of matched donations?\n\np_1_1 = df[df['ratio'] == 1]['gave'].mean()\np_2_1 = df[df['ratio'] == 2]['gave'].mean()\np_3_1 = df[df['ratio'] == 3]['gave'].mean()\n\nprint(\"Response rates:\")\nprint(f\"1:1 = {p_1_1*100:.3f}%, 2:1 = {p_2_1*100:.3f}%, 3:1 = {p_3_1*100:.3f}%\")\n\nResponse rates:\n1:1 = 2.075%, 2:1 = 2.263%, 3:1 = 2.273%\n\n\nDifferences (From Regression Coefficients):\n2:1 − 1:1 = 0.005 − 0.003 = 0.002\n3:1 − 2:1 = 0.005 − 0.005 = 0.000\nBoth the raw response rates and regression coefficients indicate that increasing the match from 1:1 to 2:1 produces a small increase in the probability of donating (about 0.2 percentage points), while moving from 2:1 to 3:1 shows virtually no change. These differences are very modest in size, and the lack of improvement from 2:1 to 3:1 suggests diminishing returns to increasing match generosity. The findings reinforce the conclusion that offering any match increases donations, but offering a larger match ratio does not lead to proportionally greater giving. The signal of a match itself may be more powerful than the actual amount matched.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\ntodo: Calculate a t-test or run a bivariate linear regression of the donation amount on the treatment status. What do we learn from doing this analysis?\n\nmodel = rsm.model.regress(data = df, rvar = 'amount', evar = 'treatment')\nmodel.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : amount\nExplanatory variables: treatment\nNull hyp.: the effect of x on amount is zero\nAlt. hyp.: the effect of x on amount is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.813      0.067   12.063  &lt; .001  ***\ntreatment        0.154      0.083    1.861   0.063    .\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: 0.0\nF-statistic: 3.461 df(1, 50081), p.value 0.063\nNr obs: 50,083\n\n\nFrom this analysis, we learn that individuals who received the matching grant treatment donated slightly more on average than those in the control group, but the difference is only marginally statistically significant (p = 0.063). The treatment effect estimate suggests that the treatment group gave about 15 cents more per person than the control group, which is a small increase. This result implies that the primary effect of the matching grant is likely driven by increasing the number of people who give, rather than substantially increasing the donation amounts of those who would already have donated. It highlights that while the match offer motivates more people to donate, it doesn’t strongly influence how much they give on average.\ntodo: now limit the data to just people who made a donation and repeat the previous analysis. This regression allows you to analyze how much respondents donate conditional on donating some positive amount. Interpret the regression coefficients – what did we learn? Does the treatment coefficient have a causal interpretation?\n\nmodel = rsm.model.regress(data = df[df['gave'] == 1], rvar = 'amount', evar = 'treatment')\nmodel.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : amount\nExplanatory variables: treatment\nNull hyp.: the effect of x on amount is zero\nAlt. hyp.: the effect of x on amount is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept       45.540      2.423   18.792  &lt; .001  ***\ntreatment       -1.668      2.872   -0.581   0.561     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.001\nF-statistic: 0.337 df(1, 1032), p.value 0.561\nNr obs: 1,034\n\n\nThe regression results show that among individuals who made a donation, the treatment group gave slightly less on average than the control group, but this difference (−1.67) is not statistically significant (p = 0.561). This tells us that the matching grant treatment did not affect the amount given by those who chose to donate. In other words, the presence of a match increased the number of people who gave, but not how much they gave once they decided to give.\nAs for causal interpretation: since this regression is limited only to people who donated, it suffers from selection bias — treatment may have influenced who donated, and the group of donors in treatment and control may differ in unobserved ways. Therefore, the treatment coefficient cannot be interpreted causally in this conditional regression. Only the earlier regression using the full sample (which preserves random assignment) supports a causal interpretation of treatment effects.\ntodo: Make two plot: one for the treatment group and one for the control. Each plot should be a histogram of the donation amounts only among people who donated. Add a red vertical bar or some other annotation to indicate the sample average for each plot.\n\ndonors = df[df['gave'] == 1]\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\nsns.histplot(donors[donors['treatment'] == 0]['amount'], bins=30, ax=axes[0], color='skyblue')\naxes[0].axvline(donors[donors['treatment'] == 0]['amount'].mean(), color='red', linestyle='--')\naxes[0].set_title('Control Group')\naxes[0].set_xlabel('Donation Amount')\n\nsns.histplot(donors[donors['treatment'] == 1]['amount'], bins=30, ax=axes[1], color='lightgreen')\naxes[1].axvline(donors[donors['treatment'] == 1]['amount'].mean(), color='red', linestyle='--')\naxes[1].set_title('Treatment Group')\naxes[1].set_xlabel('Donation Amount')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "hw1_questions.html#simulation-experiment",
    "href": "hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nto do: Make a plot like those on slide 43 from our first class and explain the plot to the reader. To do this, you will simulate 100,00 draws from the control distribution and 10,000 draws from the treatment distribution. You’ll then calculate a vector of 10,000 differences, and then you’ll plot the cumulative average of that vector of differences. Comment on whether the cumulative average approaches the true difference in means.\n\nn = 10000\np_control = 0.018\np_treatment = 0.022\ntrue_diff = p_treatment - p_control \n\ncontrol = np.random.binomial(1, p_control, n)\ntreatment = np.random.binomial(1, p_treatment, n)\n\ndiffs = treatment - control \n\ncumulative_avg = np.cumsum(diffs) / np.arange(1, n + 1)\n\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_avg, label='Cumulative Average Difference')\nplt.axhline(true_diff, color='red', linestyle='--', label='True Difference = 0.004')\nplt.title('Law of Large Numbers: Cumulative Average of Treatment - Control')\nplt.xlabel('Number of Simulated Pairs')\nplt.ylabel('Average Difference')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe cumulative average of the difference in donation outcomes between the treatment and control groups begins with high variability but quickly stabilizes as more simulated pairs are added. By around 1,000–2,000 iterations, the average difference converges very closely to the true population difference of 0.004, shown by the red dashed line. This confirms that as the sample size increases, the sample average becomes a reliable and consistent estimator of the population average. In other words, the plot clearly shows that the cumulative average approaches the true difference in means, which is exactly what the Law of Large Numbers predicts.\n\n\nCentral Limit Theorem\nto do: Make 4 histograms like those on slide 44 from our first class at sample sizes 50, 200, 500, and 1000 and explain these plots to the reader. To do this for a sample size of e.g. 50, take 50 draws from each of the control and treatment distributions, and calculate the average difference between those draws. Then repeat that process 999 more times so that you have 1000 averages. Plot the histogram of those averages. Comment on whether zero is in the “middle” of the distribution or whether it’s in the “tail.”\n\np_control = 0.018\np_treatment = 0.022\ntrue_diff = p_treatment - p_control\nn_sim = 1000\nsample_sizes = [50, 200, 500, 1000]\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\naxes = axes.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    avg_diffs = []\n\n    for _ in range(n_sim):\n        control_sample = np.random.binomial(1, p_control, n)\n        treatment_sample = np.random.binomial(1, p_treatment, n)\n        diff = treatment_sample.mean() - control_sample.mean()\n        avg_diffs.append(diff)\n    \n    sns.histplot(avg_diffs, bins=30, kde=True, stat=\"frequency\", color=\"blue\", ax=axes[i])\n    axes[i].axvline(0, color='red', linestyle='--', linewidth=2)\n    axes[i].set_title(f\"Sample size = {n}\", fontsize=14)\n    axes[i].set_xlabel(\"Mean Difference (Treatment - Control)\")\n    axes[i].set_ylabel(\"Frequency\")\n\nplt.suptitle(\"Central Limit Theorem Simulation\", fontsize=16, y=1.02)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nIn the histogram for a sample size of 50, the distribution is wide and irregular, and zero appears near the center — indicating that at this small sample size, it’s still quite common to observe no difference between treatment and control groups due to random variation. As the sample size increases to 200 and 500, the distributions become more symmetric and bell-shaped, with zero gradually shifting away from the peak. By the time we reach a sample size of 1000, the distribution is tightly centered around a positive value, and zero clearly lies in the tail of the distribution. This shift demonstrates how larger samples improve our ability to detect even small differences and reduce the likelihood of falsely concluding that there’s no effect when one actually exists. In short, as sample size increases, zero moves from the middle toward the tails, confirming the predictions of the Central Limit Theorem and the increasing statistical power of larger samples."
  },
  {
    "objectID": "blog/project1/hw1_questions.html",
    "href": "blog/project1/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/project1/hw1_questions.html#introduction",
    "href": "blog/project1/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/project1/hw1_questions.html#data",
    "href": "blog/project1/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\ntodo: Read the data into R/Python and describe the data\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pyrsm as rsm\ndf = pd.read_stata(r\"karlan_list_2007.dta\")\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\ntodo: test a few variables other than the key outcome variables (for example, test months since last donation) to see if the treatment and control groups are statistically significantly different at the 95% confidence level. Do each as a t-test and separately as a linear regression, and confirm you get the exact same results from both methods. When doing a t-test, use the formula in the class slides. When doing the linear regression, regress for example mrm2 on treatment and look at the estimated coefficient on the treatment variable. It might be helpful to compare parts of your analysis to Table 1 in the paper. Be sure to comment on your results (hint: why is Table 1 included in the paper).\n\ntreat = df[df['treatment'] == 1]['mrm2']\ncontrol = df[df['treatment'] == 0]['mrm2']\n\nmean_treat = treat.mean()\nmean_control = control.mean()\nvar_treat = treat.var(ddof=1)\nvar_control = control.var(ddof=1)\nn_treat = treat.count()\nn_control = control.count()\n\nnumerator = mean_treat - mean_control\ndenominator = np.sqrt(var_treat / n_treat + var_control / n_control)\nt_stat = numerator / denominator\n\nprint(\"Mean Treatment:\", round(mean_treat,3))\nprint(\"Std Treatment:\", round(np.sqrt(var_treat),3))\n\nprint(\"Mean Control:\", round(mean_control,3))\nprint(\"Std Control:\", round(np.sqrt(var_control),3))\nprint(\"Overall Mean:\", round(df['mrm2'].mean(),3))\nprint(\"Overall Std:\", round(df['mrm2'].std(),3))\nprint(\"t test:\", t_stat)\n\nMean Treatment: 13.012\nStd Treatment: 12.086\nMean Control: 12.998\nStd Control: 12.074\nOverall Mean: 13.007\nOverall Std: 12.081\nt test: 0.1195315522817725\n\n\n\nmodel = rsm.model.regress(data = df, rvar = 'mrm2', evar = 'treatment')\nmodel.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : mrm2\nExplanatory variables: treatment\nNull hyp.: the effect of x on mrm2 is zero\nAlt. hyp.: the effect of x on mrm2 is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept       12.998      0.094  138.979  &lt; .001  ***\ntreatment        0.014      0.115    0.119   0.905     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.0\nF-statistic: 0.014 df(1, 50080), p.value 0.905\nNr obs: 50,082 (1 obs. dropped)\n\n\nThe manual t-test and linear regression results for mrm2 (months since last donation) show no statistically significant difference between the treatment and control groups (t ≈ 0.12, p ≈ 0.905), indicating that this key pre-treatment variable is well balanced. This supports the validity of the random assignment and confirms that the experimental groups are comparable before the intervention. Table 1 is included in the paper to show that randomization produced balanced groups across a wide range of covariates, ensuring that any observed differences in outcomes can be attributed to the treatment rather than pre-existing differences."
  },
  {
    "objectID": "blog/project1/hw1_questions.html#experimental-results",
    "href": "blog/project1/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\ntodo: make a barplot with two bars. Each bar is the proportion of people who donated. One bar for treatment and one bar for control.\n\nprop_donated = df.groupby('treatment')['gave'].mean().reset_index()\nprop_donated['group'] = prop_donated['treatment'].map({0: 'Control', 1: 'Treatment'})\n\nsns.barplot(data=prop_donated, x='group', y='gave')\nplt.ylabel(\"Proportion Who Donated\")\nplt.title(\"Donation Rate by Group\")\nplt.show()\n\n\n\n\n\n\n\n\ntodo: run a t-test between the treatment and control groups on the binary outcome of whether any charitable donation was made. Also run a bivariate linear regression that demonstrates the same finding. (It may help to confirm your calculations match Table 2a Panel A.) Report your statistical results and interpret them in the context of the experiment (e.g., if you found a difference with a small p-value or that was statistically significant at some threshold, what have you learned about human behavior? Use mostly English words, not numbers or stats, to explain your finding.)\n\ntreat = df[df['treatment'] == 1]['gave']\ncontrol = df[df['treatment'] == 0]['gave']\n\nmean_diff = treat.mean() - control.mean()\nvar_treat = treat.var()\nvar_control = control.var()\nn_treat = len(treat)\nn_control = len(control)\n\nt_stat = mean_diff / np.sqrt(var_treat / n_treat + var_control / n_control)\nprint(\"T-statistic:\", t_stat)\n\nT-statistic: 3.2094621908279835\n\n\n\nmodel = rsm.model.regress(data = df, rvar = 'gave', evar = 'treatment')\nmodel.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : gave\nExplanatory variables: treatment\nNull hyp.: the effect of x on gave is zero\nAlt. hyp.: the effect of x on gave is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.018      0.001   16.225  &lt; .001  ***\ntreatment        0.004      0.001    3.101   0.002   **\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: 0.0\nF-statistic: 9.618 df(1, 50081), p.value 0.002\nNr obs: 50,083\n\n\nThe statistical analysis shows that individuals who received the treatment — a matching donation offer — were significantly more likely to make a charitable contribution compared to those who received the control letter. Both the t-test and linear regression indicate that this difference is unlikely to be due to chance. This suggests that the presence of a matching grant acts as a powerful motivator, nudging people toward taking action. In the context of human behavior, this reveals that even a subtle change in framing — like knowing one’s gift will be matched — can meaningfully increase the likelihood of giving. It highlights how social cues or perceived amplification of impact can influence decision-making in charitable contexts.\ntodo: run a probit regression where the outcome variable is whether any charitable donation was made and the explanatory variable is assignment to treatment or control. Confirm that your results replicate Table 3 column 1 in the paper.\n\nfrom statsmodels.discrete.discrete_model import Probit\n\nprobit_model = Probit.from_formula('gave ~ treatment', data=df)\nprobit_result = probit_model.fit()\nprint(probit_result.summary())\nmfx = probit_result.get_margeff()\nprint(mfx.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                   gave   No. Observations:                50083\nModel:                         Probit   Df Residuals:                    50081\nMethod:                           MLE   Df Model:                            1\nDate:                Sun, 20 Apr 2025   Pseudo R-squ.:               0.0009783\nTime:                        15:47:35   Log-Likelihood:                -5030.5\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                  0.001696\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -2.1001      0.023    -90.073      0.000      -2.146      -2.054\ntreatment      0.0868      0.028      3.113      0.002       0.032       0.141\n==============================================================================\n       Probit Marginal Effects       \n=====================================\nDep. Variable:                   gave\nMethod:                          dydx\nAt:                           overall\n==============================================================================\n                dy/dx    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\ntreatment      0.0043      0.001      3.104      0.002       0.002       0.007\n==============================================================================\n\n\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\ntodo: Use a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not. For example, does the 2:1 match rate lead increase the likelihood that someone donates as compared to the 1:1 match rate? Do your results support the “figures suggest” comment the authors make on page 8?\n\n# t-test between 2:1 and 1:1 ratios\ngave_1_1 = df[df['ratio'] == 1]['gave']\ngave_2_1 = df[df['ratio'] == 2]['gave']\n\nfrom scipy.stats import ttest_ind\nt_stat, p_val = ttest_ind(gave_2_1, gave_1_1, equal_var=False)\n\nprint(\"2:1 vs 1:1 Match Rate:\")\nprint(f\"T-stat: {t_stat:.3f}, P-value: {p_val:.3f}\")\n\n2:1 vs 1:1 Match Rate:\nT-stat: 0.965, P-value: 0.335\n\n\n\n# t-test between 3:1 and 1:1 ratios\ngave_3_1 = df[df['ratio'] == 3]['gave']\ngave_1_1 = df[df['ratio'] == 1]['gave']\n\nt_stat, p_val = ttest_ind(gave_3_1, gave_1_1, equal_var=False)\nprint(\"3:1 vs 1:1 Match Rate:\")\nprint(f\"T-stat: {t_stat:.3f}, P-value: {p_val:.3f}\")\n\n3:1 vs 1:1 Match Rate:\nT-stat: 1.015, P-value: 0.310\n\n\n\n# t-test between 3:1 and 2:1 ratios\ngave_3_1 = df[df['ratio'] == 3]['gave']\ngave_2_1 = df[df['ratio'] == 2]['gave']\n\nt_stat, p_val = ttest_ind(gave_3_1, gave_2_1, equal_var=False)\nprint(\"3:1 vs 2:1 Match Rate:\")\nprint(f\"T-stat: {t_stat:.3f}, P-value: {p_val:.3f}\")\n\n3:1 vs 2:1 Match Rate:\nT-stat: 0.050, P-value: 0.960\n\n\nMy results clearly support the authors’ conclusion: while higher match ratios may show slightly higher donation rates numerically, those differences are not statistically significant, and therefore do not provide strong evidence that larger match sizes are more effective than 1:1 matches.\ntodo: Assess the same issue using a regression. Specifically, create the variable ratio1 then regress gave on ratio1, ratio2, and ratio3 (or alternatively, regress gave on the categorical variable ratio). Interpret the coefficients and their statistical precision.\n\nmodel = rsm.model.regress(data = df, rvar = 'gave', evar = 'ratio')\nmodel.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : gave\nExplanatory variables: ratio\nNull hyp.: the effect of x on gave is zero\nAlt. hyp.: the effect of x on gave is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.018      0.001   16.225  &lt; .001  ***\nratio[1]         0.003      0.002    1.661   0.097    .\nratio[2]         0.005      0.002    2.744   0.006   **\nratio[3]         0.005      0.002    2.802   0.005   **\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: 0.0\nF-statistic: 3.665 df(3, 50079), p.value 0.012\nNr obs: 50,083\n\n\nRegression results show that all three match ratios (1:1, 2:1, 3:1) are associated with slightly higher donation rates compared to the control group, but the differences are small. The coefficient for ratio[1] (1:1 match) is 0.003 and marginally significant (p = 0.097), while the coefficients for ratio[2] (2:1) and ratio[3] (3:1) are both 0.005 and statistically significant at the 1% level (p = 0.006 and p = 0.005). However, the differences between them are not large in magnitude — all are within 0.002 of each other — suggesting that although offering any match tends to increase donations, there is little evidence that larger match sizes lead to proportionally greater increases. This supports the conclusion that the presence of a match matters more than its generosity, and that people are generally responsive to the signal of support, not necessarily the size of the match.\ntodo: Calculate the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios. Do this directly from the data, and do it by computing the differences in the fitted coefficients of the previous regression. what do you conclude regarding the effectiveness of different sizes of matched donations?\n\np_1_1 = df[df['ratio'] == 1]['gave'].mean()\np_2_1 = df[df['ratio'] == 2]['gave'].mean()\np_3_1 = df[df['ratio'] == 3]['gave'].mean()\n\nprint(\"Response rates:\")\nprint(f\"1:1 = {p_1_1*100:.3f}%, 2:1 = {p_2_1*100:.3f}%, 3:1 = {p_3_1*100:.3f}%\")\n\nResponse rates:\n1:1 = 2.075%, 2:1 = 2.263%, 3:1 = 2.273%\n\n\nDifferences (From Regression Coefficients):\n2:1 − 1:1 = 0.005 − 0.003 = 0.002\n3:1 − 2:1 = 0.005 − 0.005 = 0.000\nBoth the raw response rates and regression coefficients indicate that increasing the match from 1:1 to 2:1 produces a small increase in the probability of donating (about 0.2 percentage points), while moving from 2:1 to 3:1 shows virtually no change. These differences are very modest in size, and the lack of improvement from 2:1 to 3:1 suggests diminishing returns to increasing match generosity. The findings reinforce the conclusion that offering any match increases donations, but offering a larger match ratio does not lead to proportionally greater giving. The signal of a match itself may be more powerful than the actual amount matched.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\ntodo: Calculate a t-test or run a bivariate linear regression of the donation amount on the treatment status. What do we learn from doing this analysis?\n\nmodel = rsm.model.regress(data = df, rvar = 'amount', evar = 'treatment')\nmodel.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : amount\nExplanatory variables: treatment\nNull hyp.: the effect of x on amount is zero\nAlt. hyp.: the effect of x on amount is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.813      0.067   12.063  &lt; .001  ***\ntreatment        0.154      0.083    1.861   0.063    .\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: 0.0\nF-statistic: 3.461 df(1, 50081), p.value 0.063\nNr obs: 50,083\n\n\nFrom this analysis, we learn that individuals who received the matching grant treatment donated slightly more on average than those in the control group, but the difference is only marginally statistically significant (p = 0.063). The treatment effect estimate suggests that the treatment group gave about 15 cents more per person than the control group, which is a small increase. This result implies that the primary effect of the matching grant is likely driven by increasing the number of people who give, rather than substantially increasing the donation amounts of those who would already have donated. It highlights that while the match offer motivates more people to donate, it doesn’t strongly influence how much they give on average.\ntodo: now limit the data to just people who made a donation and repeat the previous analysis. This regression allows you to analyze how much respondents donate conditional on donating some positive amount. Interpret the regression coefficients – what did we learn? Does the treatment coefficient have a causal interpretation?\n\nmodel = rsm.model.regress(data = df[df['gave'] == 1], rvar = 'amount', evar = 'treatment')\nmodel.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : amount\nExplanatory variables: treatment\nNull hyp.: the effect of x on amount is zero\nAlt. hyp.: the effect of x on amount is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept       45.540      2.423   18.792  &lt; .001  ***\ntreatment       -1.668      2.872   -0.581   0.561     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.001\nF-statistic: 0.337 df(1, 1032), p.value 0.561\nNr obs: 1,034\n\n\nThe regression results show that among individuals who made a donation, the treatment group gave slightly less on average than the control group, but this difference (−1.67) is not statistically significant (p = 0.561). This tells us that the matching grant treatment did not affect the amount given by those who chose to donate. In other words, the presence of a match increased the number of people who gave, but not how much they gave once they decided to give.\nAs for causal interpretation: since this regression is limited only to people who donated, it suffers from selection bias — treatment may have influenced who donated, and the group of donors in treatment and control may differ in unobserved ways. Therefore, the treatment coefficient cannot be interpreted causally in this conditional regression. Only the earlier regression using the full sample (which preserves random assignment) supports a causal interpretation of treatment effects.\ntodo: Make two plot: one for the treatment group and one for the control. Each plot should be a histogram of the donation amounts only among people who donated. Add a red vertical bar or some other annotation to indicate the sample average for each plot.\n\ndonors = df[df['gave'] == 1]\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\nsns.histplot(donors[donors['treatment'] == 0]['amount'], bins=30, ax=axes[0], color='skyblue')\naxes[0].axvline(donors[donors['treatment'] == 0]['amount'].mean(), color='red', linestyle='--')\naxes[0].set_title('Control Group')\naxes[0].set_xlabel('Donation Amount')\n\nsns.histplot(donors[donors['treatment'] == 1]['amount'], bins=30, ax=axes[1], color='lightgreen')\naxes[1].axvline(donors[donors['treatment'] == 1]['amount'].mean(), color='red', linestyle='--')\naxes[1].set_title('Treatment Group')\naxes[1].set_xlabel('Donation Amount')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "blog/project1/hw1_questions.html#simulation-experiment",
    "href": "blog/project1/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nto do: Make a plot like those on slide 43 from our first class and explain the plot to the reader. To do this, you will simulate 100,00 draws from the control distribution and 10,000 draws from the treatment distribution. You’ll then calculate a vector of 10,000 differences, and then you’ll plot the cumulative average of that vector of differences. Comment on whether the cumulative average approaches the true difference in means.\n\nn = 10000\np_control = 0.018\np_treatment = 0.022\ntrue_diff = p_treatment - p_control \n\ncontrol = np.random.binomial(1, p_control, n)\ntreatment = np.random.binomial(1, p_treatment, n)\n\ndiffs = treatment - control \n\ncumulative_avg = np.cumsum(diffs) / np.arange(1, n + 1)\n\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_avg, label='Cumulative Average Difference')\nplt.axhline(true_diff, color='red', linestyle='--', label='True Difference = 0.004')\nplt.title('Law of Large Numbers: Cumulative Average of Treatment - Control')\nplt.xlabel('Number of Simulated Pairs')\nplt.ylabel('Average Difference')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe cumulative average of the difference in donation outcomes between the treatment and control groups begins with high variability but quickly stabilizes as more simulated pairs are added. By around 1,000–2,000 iterations, the average difference converges very closely to the true population difference of 0.004, shown by the red dashed line. This confirms that as the sample size increases, the sample average becomes a reliable and consistent estimator of the population average. In other words, the plot clearly shows that the cumulative average approaches the true difference in means, which is exactly what the Law of Large Numbers predicts.\n\n\nCentral Limit Theorem\nto do: Make 4 histograms like those on slide 44 from our first class at sample sizes 50, 200, 500, and 1000 and explain these plots to the reader. To do this for a sample size of e.g. 50, take 50 draws from each of the control and treatment distributions, and calculate the average difference between those draws. Then repeat that process 999 more times so that you have 1000 averages. Plot the histogram of those averages. Comment on whether zero is in the “middle” of the distribution or whether it’s in the “tail.”\n\np_control = 0.018\np_treatment = 0.022\ntrue_diff = p_treatment - p_control\nn_sim = 1000\nsample_sizes = [50, 200, 500, 1000]\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\naxes = axes.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    avg_diffs = []\n\n    for _ in range(n_sim):\n        control_sample = np.random.binomial(1, p_control, n)\n        treatment_sample = np.random.binomial(1, p_treatment, n)\n        diff = treatment_sample.mean() - control_sample.mean()\n        avg_diffs.append(diff)\n    \n    sns.histplot(avg_diffs, bins=30, kde=True, stat=\"frequency\", color=\"blue\", ax=axes[i])\n    axes[i].axvline(0, color='red', linestyle='--', linewidth=2)\n    axes[i].set_title(f\"Sample size = {n}\", fontsize=14)\n    axes[i].set_xlabel(\"Mean Difference (Treatment - Control)\")\n    axes[i].set_ylabel(\"Frequency\")\n\nplt.suptitle(\"Central Limit Theorem Simulation\", fontsize=16, y=1.02)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nIn the histogram for a sample size of 50, the distribution is wide and irregular, and zero appears near the center — indicating that at this small sample size, it’s still quite common to observe no difference between treatment and control groups due to random variation. As the sample size increases to 200 and 500, the distributions become more symmetric and bell-shaped, with zero gradually shifting away from the peak. By the time we reach a sample size of 1000, the distribution is tightly centered around a positive value, and zero clearly lies in the tail of the distribution. This shift demonstrates how larger samples improve our ability to detect even small differences and reduce the likelihood of falsely concluding that there’s no effect when one actually exists. In short, as sample size increases, zero moves from the middle toward the tails, confirming the predictions of the Central Limit Theorem and the increasing statistical power of larger samples."
  },
  {
    "objectID": "blog/project1/index.html",
    "href": "blog/project1/index.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/project1/index.html#introduction",
    "href": "blog/project1/index.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/project1/index.html#data",
    "href": "blog/project1/index.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\nI begin by reading the dataset into Python and generating descriptive statistics to understand its structure.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pyrsm as rsm\ndf = pd.read_stata(r\"karlan_list_2007.dta\")\ndf.describe()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio2\nratio3\nsize25\nsize50\nsize100\nsizeno\naskd1\naskd2\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\ncount\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n...\n49978.000000\n49978.000000\n48217.000000\n48047.000000\n48217.000000\n48221.000000\n48209.000000\n48214.000000\n48215.000000\n48217.000000\n\n\nmean\n0.666813\n0.333187\n0.222311\n0.222211\n0.166723\n0.166623\n0.166723\n0.166743\n0.222311\n0.222291\n...\n0.510245\n0.488715\n0.819599\n0.086710\n0.321694\n2.429012\n54815.700533\n0.669418\n0.391661\n0.871968\n\n\nstd\n0.471357\n0.471357\n0.415803\n0.415736\n0.372732\n0.372643\n0.372732\n0.372750\n0.415803\n0.415790\n...\n0.499900\n0.499878\n0.168561\n0.135868\n0.103039\n0.378115\n22027.316665\n0.193405\n0.186599\n0.258654\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.009418\n0.000000\n0.000000\n0.000000\n5000.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.755845\n0.014729\n0.258311\n2.210000\n39181.000000\n0.560222\n0.235647\n0.884929\n\n\n50%\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n0.000000\n0.872797\n0.036554\n0.305534\n2.440000\n50673.000000\n0.712296\n0.373744\n1.000000\n\n\n75%\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n1.000000\n0.938827\n0.090882\n0.369132\n2.660000\n66005.000000\n0.816798\n0.530036\n1.000000\n\n\nmax\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n...\n1.000000\n1.000000\n1.000000\n0.989622\n0.997544\n5.270000\n200001.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n8 rows × 48 columns\n\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\nI test whether the treatment and control groups differ on pre-treatment variables like mrm2 (months since last donation) using both a t-test and a linear regression. Both methods give the same result: no significant difference. This confirms that the groups are balanced, as also shown in Table 1 of the paper.\n\n\n“mrm2”\n\ntreat = df[df['treatment'] == 1]['mrm2']\ncontrol = df[df['treatment'] == 0]['mrm2']\n\nmean_treat = treat.mean()\nmean_control = control.mean()\nvar_treat = treat.var(ddof=1)\nvar_control = control.var(ddof=1)\nn_treat = treat.count()\nn_control = control.count()\n\nnumerator = mean_treat - mean_control\ndenominator = np.sqrt(var_treat / n_treat + var_control / n_control)\nt_stat = numerator / denominator\n\nprint(\"Mean Treatment:\", round(mean_treat,3))\nprint(\"Std Treatment:\", round(np.sqrt(var_treat),3))\n\nprint(\"Mean Control:\", round(mean_control,3))\nprint(\"Std Control:\", round(np.sqrt(var_control),3))\nprint(\"Overall Mean:\", round(df['mrm2'].mean(),3))\nprint(\"Overall Std:\", round(df['mrm2'].std(),3))\nprint(\"t test:\", t_stat)\n\nMean Treatment: 13.012\nStd Treatment: 12.086\nMean Control: 12.998\nStd Control: 12.074\nOverall Mean: 13.007\nOverall Std: 12.081\nt test: 0.1195315522817725\n\n\n\nmodel = rsm.model.regress(data = df, rvar = 'mrm2', evar = 'treatment')\nmodel.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : mrm2\nExplanatory variables: treatment\nNull hyp.: the effect of x on mrm2 is zero\nAlt. hyp.: the effect of x on mrm2 is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept       12.998      0.094  138.979  &lt; .001  ***\ntreatment        0.014      0.115    0.119   0.905     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.0\nF-statistic: 0.014 df(1, 50080), p.value 0.905\nNr obs: 50,082 (1 obs. dropped)\n\n\nThe manual t-test and linear regression results for mrm2 (months since last donation) show no statistically significant difference between the treatment and control groups (t ≈ 0.12, p ≈ 0.905), indicating that this key pre-treatment variable is well balanced. This supports the validity of the random assignment and confirms that the experimental groups are comparable before the intervention. Table 1 is included in the paper to show that randomization produced balanced groups across a wide range of covariates, ensuring that any observed differences in outcomes can be attributed to the treatment rather than pre-existing differences.\n\n\n“hpa”\n\ntreat = df[df['treatment'] == 1]['hpa']\ncontrol = df[df['treatment'] == 0]['hpa']\n\nmean_treat = treat.mean()\nmean_control = control.mean()\nvar_treat = treat.var(ddof=1)\nvar_control = control.var(ddof=1)\nn_treat = treat.count()\nn_control = control.count()\n\nnumerator = mean_treat - mean_control\ndenominator = np.sqrt(var_treat / n_treat + var_control / n_control)\nt_stat = numerator / denominator\n\nprint(\"Mean Treatment:\", round(mean_treat,3))\nprint(\"Std Treatment:\", round(np.sqrt(var_treat),3))\n\nprint(\"Mean Control:\", round(mean_control,3))\nprint(\"Std Control:\", round(np.sqrt(var_control),3))\nprint(\"Overall Mean:\", round(df['hpa'].mean(),3))\nprint(\"Overall Std:\", round(df['hpa'].std(),3))\nprint(\"t test:\", t_stat)\n\nMean Treatment: 59.597\nStd Treatment: 73.052\nMean Control: 58.96\nStd Control: 67.269\nOverall Mean: 59.385\nOverall Std: 71.177\nt test: 0.9704273516144837\n\n\n\nmodel = rsm.model.regress(data = df, rvar = 'hpa', evar = 'treatment')\nmodel.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : hpa\nExplanatory variables: treatment\nNull hyp.: the effect of x on hpa is zero\nAlt. hyp.: the effect of x on hpa is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept       58.960      0.551  107.005  &lt; .001  ***\ntreatment        0.637      0.675    0.944   0.345     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.0\nF-statistic: 0.891 df(1, 50081), p.value 0.345\nNr obs: 50,083\n\n\nThe hpa variable is a pre-treatment covariate, so the lack of a significant difference across groups provides evidence that random assignment was successful. In particular, it suggests that prior donation behavior (as measured by the highest past contribution) was balanced across the treatment and control conditions. This supports the validity of any causal claims made later in the experiment, as we can reasonably assume that any post-treatment differences in donation behavior are due to the treatment itself rather than pre-existing differences in donor generosity.\n\n\n“freq”\n\ntreat = df[df['treatment'] == 1]['freq']\ncontrol = df[df['treatment'] == 0]['freq']\n\nmean_treat = treat.mean()\nmean_control = control.mean()\nvar_treat = treat.var(ddof=1)\nvar_control = control.var(ddof=1)\nn_treat = treat.count()\nn_control = control.count()\n\nnumerator = mean_treat - mean_control\ndenominator = np.sqrt(var_treat / n_treat + var_control / n_control)\nt_stat = numerator / denominator\n\nprint(\"Mean Treatment:\", round(mean_treat,3))\nprint(\"Std Treatment:\", round(np.sqrt(var_treat),3))\n\nprint(\"Mean Control:\", round(mean_control,3))\nprint(\"Std Control:\", round(np.sqrt(var_control),3))\nprint(\"Overall Mean:\", round(df['freq'].mean(),3))\nprint(\"Overall Std:\", round(df['freq'].std(),3))\nprint(\"t test:\", t_stat)\n\nMean Treatment: 8.035\nStd Treatment: 11.39\nMean Control: 8.047\nStd Control: 11.404\nOverall Mean: 8.039\nOverall Std: 11.394\nt test: -0.11084502380904246\n\n\n\nmodel = rsm.model.regress(data = df, rvar = 'freq', evar = 'treatment')\nmodel.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : freq\nExplanatory variables: treatment\nNull hyp.: the effect of x on freq is zero\nAlt. hyp.: the effect of x on freq is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        8.047      0.088   91.231  &lt; .001  ***\ntreatment       -0.012      0.108   -0.111   0.912     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.0\nF-statistic: 0.012 df(1, 50081), p.value 0.912\nNr obs: 50,083\n\n\nThe variable freq, representing the number of prior donations, shows nearly identical means between the treatment and control groups. Both the manual t-test and regression confirm that this difference is not statistically significant. This suggests that random assignment successfully balanced this pre-treatment characteristic, which supports the internal validity of the experimental design. Because the treatment and control groups are statistically equivalent in terms of donation frequency history, we can be more confident that any observed post-treatment effects on donation behavior are attributable to the treatment rather than pre-existing differences in donor activity."
  },
  {
    "objectID": "blog/project1/index.html#experimental-results",
    "href": "blog/project1/index.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\nI create a barplot showing the proportion of people who donated in each group. One bar represents the treatment group and the other represents the control group, allowing for a visual comparison of donation rates.\n\nprop_donated = df.groupby('treatment')['gave'].mean().reset_index()\nprop_donated['group'] = prop_donated['treatment'].map({0: 'Control', 1: 'Treatment'})\n\nsns.barplot(data=prop_donated, x='group', y='gave')\nplt.ylabel(\"Proportion Who Donated\")\nplt.title(\"Donation Rate by Group\")\nplt.show()\n\n\n\n\n\n\n\n\nI run a t-test and a linear regression to compare donation rates between treatment and control. Both show that the treatment group is more likely to donate. This suggests that people respond to matching offers by being more willing to give.\n\ntreat = df[df['treatment'] == 1]['gave']\ncontrol = df[df['treatment'] == 0]['gave']\n\nmean_diff = treat.mean() - control.mean()\nvar_treat = treat.var()\nvar_control = control.var()\nn_treat = len(treat)\nn_control = len(control)\n\nt_stat = mean_diff / np.sqrt(var_treat / n_treat + var_control / n_control)\nprint(\"T-statistic:\", t_stat)\n\nT-statistic: 3.2094621908279835\n\n\n\nmodel = rsm.model.regress(data = df, rvar = 'gave', evar = 'treatment')\nmodel.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : gave\nExplanatory variables: treatment\nNull hyp.: the effect of x on gave is zero\nAlt. hyp.: the effect of x on gave is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.018      0.001   16.225  &lt; .001  ***\ntreatment        0.004      0.001    3.101   0.002   **\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: 0.0\nF-statistic: 9.618 df(1, 50081), p.value 0.002\nNr obs: 50,083\n\n\nThe statistical analysis shows that individuals who received the treatment — a matching donation offer — were significantly more likely to make a charitable contribution compared to those who received the control letter. Both the t-test and linear regression indicate that this difference is unlikely to be due to chance. This suggests that the presence of a matching grant acts as a powerful motivator, nudging people toward taking action. In the context of human behavior, this reveals that even a subtle change in framing — like knowing one’s gift will be matched — can meaningfully increase the likelihood of giving. It highlights how social cues or perceived amplification of impact can influence decision-making in charitable contexts.\nI run a probit regression of donation on treatment assignment. The results replicate Table 3, column 1 in the paper, showing a positive and significant effect of treatment on the likelihood of donating.\n\nfrom statsmodels.discrete.discrete_model import Probit\n\nprobit_model = Probit.from_formula('gave ~ treatment', data=df)\nprobit_result = probit_model.fit()\nprint(probit_result.summary())\nmfx = probit_result.get_margeff()\nprint(mfx.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                   gave   No. Observations:                50083\nModel:                         Probit   Df Residuals:                    50081\nMethod:                           MLE   Df Model:                            1\nDate:                Tue, 06 May 2025   Pseudo R-squ.:               0.0009783\nTime:                        12:05:41   Log-Likelihood:                -5030.5\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                  0.001696\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -2.1001      0.023    -90.073      0.000      -2.146      -2.054\ntreatment      0.0868      0.028      3.113      0.002       0.032       0.141\n==============================================================================\n       Probit Marginal Effects       \n=====================================\nDep. Variable:                   gave\nMethod:                          dydx\nAt:                           overall\n==============================================================================\n                dy/dx    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\ntreatment      0.0043      0.001      3.104      0.002       0.002       0.007\n==============================================================================\n\n\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\nI run t-tests comparing donation rates between different match ratios (1:1, 2:1, 3:1). The results show no significant differences, suggesting that larger match ratios do not increase donation likelihood. This supports the authors’ comment that higher match rates don’t have additional impact beyond offering a match.\n\n# t-test between 2:1 and 1:1 ratios\ngave_1_1 = df[df['ratio'] == 1]['gave']\ngave_2_1 = df[df['ratio'] == 2]['gave']\n\nfrom scipy.stats import ttest_ind\nt_stat, p_val = ttest_ind(gave_2_1, gave_1_1, equal_var=False)\n\nprint(\"2:1 vs 1:1 Match Rate:\")\nprint(f\"T-stat: {t_stat:.3f}, P-value: {p_val:.3f}\")\n\n2:1 vs 1:1 Match Rate:\nT-stat: 0.965, P-value: 0.335\n\n\n\n# t-test between 3:1 and 1:1 ratios\ngave_3_1 = df[df['ratio'] == 3]['gave']\ngave_1_1 = df[df['ratio'] == 1]['gave']\n\nt_stat, p_val = ttest_ind(gave_3_1, gave_1_1, equal_var=False)\nprint(\"3:1 vs 1:1 Match Rate:\")\nprint(f\"T-stat: {t_stat:.3f}, P-value: {p_val:.3f}\")\n\n3:1 vs 1:1 Match Rate:\nT-stat: 1.015, P-value: 0.310\n\n\n\n# t-test between 3:1 and 2:1 ratios\ngave_3_1 = df[df['ratio'] == 3]['gave']\ngave_2_1 = df[df['ratio'] == 2]['gave']\n\nt_stat, p_val = ttest_ind(gave_3_1, gave_2_1, equal_var=False)\nprint(\"3:1 vs 2:1 Match Rate:\")\nprint(f\"T-stat: {t_stat:.3f}, P-value: {p_val:.3f}\")\n\n3:1 vs 2:1 Match Rate:\nT-stat: 0.050, P-value: 0.960\n\n\nMy results clearly support the authors’ conclusion: while higher match ratios may show slightly higher donation rates numerically, those differences are not statistically significant, and therefore do not provide strong evidence that larger match sizes are more effective than 1:1 matches.\nI run a regression of gave on the match ratio indicators. The coefficients are small, and only the 2:1 and 3:1 ratios are statistically significant, but the differences between them are minimal. This suggests that while offering a match matters, increasing the match size has little additional effect.\n\nmodel = rsm.model.regress(data = df, rvar = 'gave', evar = 'ratio')\nmodel.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : gave\nExplanatory variables: ratio\nNull hyp.: the effect of x on gave is zero\nAlt. hyp.: the effect of x on gave is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.018      0.001   16.225  &lt; .001  ***\nratio[1]         0.003      0.002    1.661   0.097    .\nratio[2]         0.005      0.002    2.744   0.006   **\nratio[3]         0.005      0.002    2.802   0.005   **\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: 0.0\nF-statistic: 3.665 df(3, 50079), p.value 0.012\nNr obs: 50,083\n\n\nRegression results show that all three match ratios (1:1, 2:1, 3:1) are associated with slightly higher donation rates compared to the control group, but the differences are small. The coefficient for ratio[1] (1:1 match) is 0.003 and marginally significant (p = 0.097), while the coefficients for ratio[2] (2:1) and ratio[3] (3:1) are both 0.005 and statistically significant at the 1% level (p = 0.006 and p = 0.005). However, the differences between them are not large in magnitude — all are within 0.002 of each other — suggesting that although offering any match tends to increase donations, there is little evidence that larger match sizes lead to proportionally greater increases. This supports the conclusion that the presence of a match matters more than its generosity, and that people are generally responsive to the signal of support, not necessarily the size of the match.\nI calculate the response rate differences both directly from the data and using regression coefficients. The difference between 1:1 and 2:1 is small, and there is no difference between 2:1 and 3:1. This suggests that increasing the match ratio does not meaningfully improve donation rates.\n\np_1_1 = df[df['ratio'] == 1]['gave'].mean()\np_2_1 = df[df['ratio'] == 2]['gave'].mean()\np_3_1 = df[df['ratio'] == 3]['gave'].mean()\n\nprint(\"Response rates:\")\nprint(f\"1:1 = {p_1_1*100:.3f}%, 2:1 = {p_2_1*100:.3f}%, 3:1 = {p_3_1*100:.3f}%\")\n\nResponse rates:\n1:1 = 2.075%, 2:1 = 2.263%, 3:1 = 2.273%\n\n\nDifferences (From Regression Coefficients):\n2:1 − 1:1 = 0.005 − 0.003 = 0.002\n3:1 − 2:1 = 0.005 − 0.005 = 0.000\nBoth the raw response rates and regression coefficients indicate that increasing the match from 1:1 to 2:1 produces a small increase in the probability of donating (about 0.2 percentage points), while moving from 2:1 to 3:1 shows virtually no change. These differences are very modest in size, and the lack of improvement from 2:1 to 3:1 suggests diminishing returns to increasing match generosity. The findings reinforce the conclusion that offering any match increases donations, but offering a larger match ratio does not lead to proportionally greater giving. The signal of a match itself may be more powerful than the actual amount matched.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\nI run a t-test and a regression of donation amount on treatment. The treatment group gives slightly more on average, but the difference is only marginally significant. This suggests that the treatment mainly increases the number of donors, not the amount they give.\n\nmodel = rsm.model.regress(data = df, rvar = 'amount', evar = 'treatment')\nmodel.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : amount\nExplanatory variables: treatment\nNull hyp.: the effect of x on amount is zero\nAlt. hyp.: the effect of x on amount is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.813      0.067   12.063  &lt; .001  ***\ntreatment        0.154      0.083    1.861   0.063    .\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: 0.0\nF-statistic: 3.461 df(1, 50081), p.value 0.063\nNr obs: 50,083\n\n\nFrom this analysis, we learn that individuals who received the matching grant treatment donated slightly more on average than those in the control group, but the difference is only marginally statistically significant (p = 0.063). The treatment effect estimate suggests that the treatment group gave about 15 cents more per person than the control group, which is a small increase. This result implies that the primary effect of the matching grant is likely driven by increasing the number of people who give, rather than substantially increasing the donation amounts of those who would already have donated. It highlights that while the match offer motivates more people to donate, it doesn’t strongly influence how much they give on average.\nI repeat the regression using only those who donated. The treatment has no significant effect on the amount given among donors. This means the treatment influences whether people donate, but not how much they give once they do. Since the sample is restricted to donors, the treatment effect here does not have a causal interpretation.\n\nmodel = rsm.model.regress(data = df[df['gave'] == 1], rvar = 'amount', evar = 'treatment')\nmodel.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : amount\nExplanatory variables: treatment\nNull hyp.: the effect of x on amount is zero\nAlt. hyp.: the effect of x on amount is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept       45.540      2.423   18.792  &lt; .001  ***\ntreatment       -1.668      2.872   -0.581   0.561     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.001\nF-statistic: 0.337 df(1, 1032), p.value 0.561\nNr obs: 1,034\n\n\nThe regression results show that among individuals who made a donation, the treatment group gave slightly less on average than the control group, but this difference (−1.67) is not statistically significant (p = 0.561). This tells us that the matching grant treatment did not affect the amount given by those who chose to donate. In other words, the presence of a match increased the number of people who gave, but not how much they gave once they decided to give.\nAs for causal interpretation: since this regression is limited only to people who donated, it suffers from selection bias — treatment may have influenced who donated, and the group of donors in treatment and control may differ in unobserved ways. Therefore, the treatment coefficient cannot be interpreted causally in this conditional regression. Only the earlier regression using the full sample (which preserves random assignment) supports a causal interpretation of treatment effects.\nI create two histograms showing the distribution of donation amounts among donors, one for the treatment group and one for the control group. Each plot includes a red vertical line indicating the group’s average donation. The distributions and averages are similar, showing that donation amounts are not affected by the treatment.\n\ndonors = df[df['gave'] == 1]\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\nsns.histplot(donors[donors['treatment'] == 0]['amount'], bins=30, ax=axes[0], color='skyblue')\naxes[0].axvline(donors[donors['treatment'] == 0]['amount'].mean(), color='red', linestyle='--')\naxes[0].set_title('Control Group')\naxes[0].set_xlabel('Donation Amount')\n\nsns.histplot(donors[donors['treatment'] == 1]['amount'], bins=30, ax=axes[1], color='lightgreen')\naxes[1].axvline(donors[donors['treatment'] == 1]['amount'].mean(), color='red', linestyle='--')\naxes[1].set_title('Treatment Group')\naxes[1].set_xlabel('Donation Amount')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "blog/project1/index.html#simulation-experiment",
    "href": "blog/project1/index.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nI simulate 100,000 draws from the control group and 10,000 from the treatment group, then calculate a vector of 10,000 differences. I plot the cumulative average of these differences to visualize how it stabilizes over time. The plot shows that the cumulative average approaches the true difference in means, illustrating the Law of Large Numbers.\n\nn = 10000\np_control = 0.018\np_treatment = 0.022\ntrue_diff = p_treatment - p_control \n\ncontrol = np.random.binomial(1, p_control, n)\ntreatment = np.random.binomial(1, p_treatment, n)\n\ndiffs = treatment - control \n\ncumulative_avg = np.cumsum(diffs) / np.arange(1, n + 1)\n\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_avg, label='Cumulative Average Difference')\nplt.axhline(true_diff, color='red', linestyle='--', label='True Difference = 0.004')\nplt.title('Law of Large Numbers: Cumulative Average of Treatment - Control')\nplt.xlabel('Number of Simulated Pairs')\nplt.ylabel('Average Difference')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe cumulative average of the difference in donation outcomes between the treatment and control groups begins with high variability but quickly stabilizes as more simulated pairs are added. By around 1,000–2,000 iterations, the average difference converges very closely to the true population difference of 0.004, shown by the red dashed line. This confirms that as the sample size increases, the sample average becomes a reliable and consistent estimator of the population average. In other words, the plot clearly shows that the cumulative average approaches the true difference in means, which is exactly what the Law of Large Numbers predicts.\n\n\nCentral Limit Theorem\nI simulate 1,000 average differences between treatment and control groups at sample sizes of 50, 200, 500, and 1000. For each sample size, I plot a histogram of the average differences. At smaller sample sizes, the distribution is wider and zero is near the center, but as the sample size increases, the distribution becomes tighter and more normal-shaped, and zero shifts into the tail. This demonstrates the Central Limit Theorem and how larger samples make it easier to detect true differences.\n\np_control = 0.018\np_treatment = 0.022\ntrue_diff = p_treatment - p_control\nn_sim = 1000\nsample_sizes = [50, 200, 500, 1000]\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\naxes = axes.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    avg_diffs = []\n\n    for _ in range(n_sim):\n        control_sample = np.random.binomial(1, p_control, n)\n        treatment_sample = np.random.binomial(1, p_treatment, n)\n        diff = treatment_sample.mean() - control_sample.mean()\n        avg_diffs.append(diff)\n    \n    sns.histplot(avg_diffs, bins=30, kde=True, stat=\"frequency\", color=\"blue\", ax=axes[i])\n    axes[i].axvline(0, color='red', linestyle='--', linewidth=2)\n    axes[i].set_title(f\"Sample size = {n}\", fontsize=14)\n    axes[i].set_xlabel(\"Mean Difference (Treatment - Control)\")\n    axes[i].set_ylabel(\"Frequency\")\n\nplt.suptitle(\"Central Limit Theorem Simulation\", fontsize=16, y=1.02)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nIn the histogram for a sample size of 50, the distribution is wide and irregular, and zero appears near the center — indicating that at this small sample size, it’s still quite common to observe no difference between treatment and control groups due to random variation. As the sample size increases to 200 and 500, the distributions become more symmetric and bell-shaped, with zero gradually shifting away from the peak. By the time we reach a sample size of 1000, the distribution is tightly centered around a positive value, and zero clearly lies in the tail of the distribution. This shift demonstrates how larger samples improve our ability to detect even small differences and reduce the likelihood of falsely concluding that there’s no effect when one actually exists. In short, as sample size increases, zero moves from the middle toward the tails, confirming the predictions of the Central Limit Theorem and the increasing statistical power of larger samples."
  },
  {
    "objectID": "blog/project2/index.html",
    "href": "blog/project2/index.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nRead in data.\n\nimport pandas as pd\ndf = pd.read_csv('blueprinty.csv')\n\nCompare histograms and means of number of patents by customer status. What do we observe?\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pyrsm as rsm\n\ncustomers = df[df['iscustomer'] == 1]['patents']\nnon_customers = df[df['iscustomer'] == 0]['patents']\n\nmean_customers = customers.mean()\nmean_non_customers = non_customers.mean()\n\nplt.figure(figsize=(10, 6))\nplt.hist(customers, bins=range(0, max(df['patents'])+2), alpha=0.7, label='Customers')\nplt.hist(non_customers, bins=range(0, max(df['patents'])+2), alpha=0.7, label='Non-Customers')\nplt.axvline(mean_customers, color='blue', linestyle='dashed', linewidth=2, label=f'Mean (Customers): {mean_customers:.2f}')\nplt.axvline(mean_non_customers, color='red', linestyle='dashed', linewidth=2, label=f'Mean (Non-Customers): {mean_non_customers:.2f}')\nplt.xlabel('Number of Patents')\nplt.ylabel('Number of Firms')\nplt.title('Patent Counts by Blueprinty Customer Status')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe histogram shows that firms using Blueprinty’s software (customers) generally have a higher mean number of patents than non-customers. Specifically:\nCustomers have a visibly higher mean number of patents.\nThe distribution for customers is skewed toward higher patent counts compared to non-customers.\nNon-customers are more concentrated in the lower patent count range.\nThis suggests an association between using Blueprinty’s software and having more patents. However, keep in mind this is correlational, not causal—other factors may influence this relationship.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\nCompare regions and ages by customer status. What do we observe?\n\nregion_counts = pd.crosstab(df['region'], df['iscustomer'], normalize='index') * 100\nregion_counts.columns = ['Non-Customers (%)', 'Customers (%)']\nregion_counts = region_counts.sort_index()\n\nage_by_customer = df.groupby('iscustomer')['age'].describe()\n\nregion_counts.plot(kind='bar', stacked=True, figsize=(10, 6))\nplt.title('Regional Distribution by Customer Status')\nplt.ylabel('Percentage of Firms')\nplt.xlabel('Region')\nplt.legend(title='Customer Status')\nplt.tight_layout()\nplt.show()\n\nage_by_customer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\niscustomer\n\n\n\n\n\n\n\n\n\n\n\n\n0\n1019.0\n26.101570\n6.945426\n9.0\n21.0\n25.5\n31.25\n47.5\n\n\n1\n481.0\n26.900208\n7.814678\n10.0\n20.5\n26.5\n32.50\n49.0\n\n\n\n\n\n\n\n\n\n\nThe stacked bar chart shows clear differences in regional distribution between customers and non-customers.\nSome regions have a higher proportion of Blueprinty customers, while others have more non-customers.\nThis suggests that region may be a confounding factor when analyzing patent outcomes.\n\n\n\nCustomers have a slightly higher average age (26.9 years) compared to non-customers (26.1 years).\nThe age distribution is fairly similar, though customers also have a slightly higher standard deviation, indicating more variability in age.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nMathematically the likelihood for \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\n\n\n\nNote that the probability mass function is given by:\n\\[\nf(Y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nFor ( n ) independent observations ( Y_1, Y_2, , Y_n ), the likelihood function is:\n\\[\nL(\\lambda; Y_1, \\dots, Y_n) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n= e^{-n\\lambda} \\lambda^{\\sum_{i=1}^n Y_i} \\prod_{i=1}^n \\frac{1}{Y_i!}\n\\]\nThe corresponding log-likelihood is:\n\\[\n\\ell(\\lambda) = \\log L(\\lambda) = -n\\lambda + \\left(\\sum_{i=1}^n Y_i\\right) \\log \\lambda - \\sum_{i=1}^n \\log(Y_i!)\n\\]\nThe following code defines a log-likelihood function for the Poisson model. It takes a value of 𝜆 and a vector of observed counts 𝑌, and calculates how likely the observed data are under that parameter. This function will later be used for estimating the most likely value of 𝜆 given the data, using maximum likelihood estimation.\n\nfrom scipy.special import gammaln\n\ndef poisson_loglikelihood(lambda_, Y):\n    if lambda_ &lt;= 0:\n        return -np.inf \n    n = len(Y)\n    sum_Y = np.sum(Y)\n    log_likelihood = -n * lambda_ + sum_Y * np.log(lambda_) - np.sum(gammaln(Y + 1))\n    return log_likelihood\n\nThe following code uses the log-likelihood function to plot how the likelihood changes across a range of λ values. It helps visualize which values of λ fit the observed data best by showing where the log-likelihood reaches its peak.\n\nY = df['patents'].values\n\nlambda_values = np.linspace(0.1, 10, 200) \n\nlog_likelihoods = [poisson_loglikelihood(lam, Y) for lam in lambda_values]\n\nplt.figure(figsize=(10, 6))\nplt.plot(lambda_values, log_likelihoods, label='Log-Likelihood')\nplt.xlabel(r'$\\lambda$')\nplt.ylabel('Log-Likelihood')\nplt.title('Poisson Log-Likelihood for Patent Counts')\nplt.grid(True)\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n_ below takes a mathematical approach to find the maximum likelihood estimate (MLE) for λ. By taking the derivative of the log-likelihood, setting it to zero, and solving, we show that the MLE for λ is simply the sample mean of the data. This result aligns with the fact that the Poisson distribution’s mean is equal to 𝜆._"
  },
  {
    "objectID": "blog/project2/index.html#blueprinty-case-study",
    "href": "blog/project2/index.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nRead in data.\n\nimport pandas as pd\ndf = pd.read_csv('blueprinty.csv')\n\nCompare histograms and means of number of patents by customer status. What do we observe?\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pyrsm as rsm\n\ncustomers = df[df['iscustomer'] == 1]['patents']\nnon_customers = df[df['iscustomer'] == 0]['patents']\n\nmean_customers = customers.mean()\nmean_non_customers = non_customers.mean()\n\nplt.figure(figsize=(10, 6))\nplt.hist(customers, bins=range(0, max(df['patents'])+2), alpha=0.7, label='Customers')\nplt.hist(non_customers, bins=range(0, max(df['patents'])+2), alpha=0.7, label='Non-Customers')\nplt.axvline(mean_customers, color='blue', linestyle='dashed', linewidth=2, label=f'Mean (Customers): {mean_customers:.2f}')\nplt.axvline(mean_non_customers, color='red', linestyle='dashed', linewidth=2, label=f'Mean (Non-Customers): {mean_non_customers:.2f}')\nplt.xlabel('Number of Patents')\nplt.ylabel('Number of Firms')\nplt.title('Patent Counts by Blueprinty Customer Status')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe histogram shows that firms using Blueprinty’s software (customers) generally have a higher mean number of patents than non-customers. Specifically:\nCustomers have a visibly higher mean number of patents.\nThe distribution for customers is skewed toward higher patent counts compared to non-customers.\nNon-customers are more concentrated in the lower patent count range.\nThis suggests an association between using Blueprinty’s software and having more patents. However, keep in mind this is correlational, not causal—other factors may influence this relationship.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\nCompare regions and ages by customer status. What do we observe?\n\nregion_counts = pd.crosstab(df['region'], df['iscustomer'], normalize='index') * 100\nregion_counts.columns = ['Non-Customers (%)', 'Customers (%)']\nregion_counts = region_counts.sort_index()\n\nage_by_customer = df.groupby('iscustomer')['age'].describe()\n\nregion_counts.plot(kind='bar', stacked=True, figsize=(10, 6))\nplt.title('Regional Distribution by Customer Status')\nplt.ylabel('Percentage of Firms')\nplt.xlabel('Region')\nplt.legend(title='Customer Status')\nplt.tight_layout()\nplt.show()\n\nage_by_customer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\niscustomer\n\n\n\n\n\n\n\n\n\n\n\n\n0\n1019.0\n26.101570\n6.945426\n9.0\n21.0\n25.5\n31.25\n47.5\n\n\n1\n481.0\n26.900208\n7.814678\n10.0\n20.5\n26.5\n32.50\n49.0\n\n\n\n\n\n\n\n\n\n\nThe stacked bar chart shows clear differences in regional distribution between customers and non-customers.\nSome regions have a higher proportion of Blueprinty customers, while others have more non-customers.\nThis suggests that region may be a confounding factor when analyzing patent outcomes.\n\n\n\nCustomers have a slightly higher average age (26.9 years) compared to non-customers (26.1 years).\nThe age distribution is fairly similar, though customers also have a slightly higher standard deviation, indicating more variability in age.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nMathematically the likelihood for \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\n\n\n\nNote that the probability mass function is given by:\n\\[\nf(Y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nFor ( n ) independent observations ( Y_1, Y_2, , Y_n ), the likelihood function is:\n\\[\nL(\\lambda; Y_1, \\dots, Y_n) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n= e^{-n\\lambda} \\lambda^{\\sum_{i=1}^n Y_i} \\prod_{i=1}^n \\frac{1}{Y_i!}\n\\]\nThe corresponding log-likelihood is:\n\\[\n\\ell(\\lambda) = \\log L(\\lambda) = -n\\lambda + \\left(\\sum_{i=1}^n Y_i\\right) \\log \\lambda - \\sum_{i=1}^n \\log(Y_i!)\n\\]\nThe following code defines a log-likelihood function for the Poisson model. It takes a value of 𝜆 and a vector of observed counts 𝑌, and calculates how likely the observed data are under that parameter. This function will later be used for estimating the most likely value of 𝜆 given the data, using maximum likelihood estimation.\n\nfrom scipy.special import gammaln\n\ndef poisson_loglikelihood(lambda_, Y):\n    if lambda_ &lt;= 0:\n        return -np.inf \n    n = len(Y)\n    sum_Y = np.sum(Y)\n    log_likelihood = -n * lambda_ + sum_Y * np.log(lambda_) - np.sum(gammaln(Y + 1))\n    return log_likelihood\n\nThe following code uses the log-likelihood function to plot how the likelihood changes across a range of λ values. It helps visualize which values of λ fit the observed data best by showing where the log-likelihood reaches its peak.\n\nY = df['patents'].values\n\nlambda_values = np.linspace(0.1, 10, 200) \n\nlog_likelihoods = [poisson_loglikelihood(lam, Y) for lam in lambda_values]\n\nplt.figure(figsize=(10, 6))\nplt.plot(lambda_values, log_likelihoods, label='Log-Likelihood')\nplt.xlabel(r'$\\lambda$')\nplt.ylabel('Log-Likelihood')\nplt.title('Poisson Log-Likelihood for Patent Counts')\nplt.grid(True)\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n_ below takes a mathematical approach to find the maximum likelihood estimate (MLE) for λ. By taking the derivative of the log-likelihood, setting it to zero, and solving, we show that the MLE for λ is simply the sample mean of the data. This result aligns with the fact that the Poisson distribution’s mean is equal to 𝜆._"
  },
  {
    "objectID": "blog/project2/index.html#deriving-the-mle-for-the-poisson-rate-parameter",
    "href": "blog/project2/index.html#deriving-the-mle-for-the-poisson-rate-parameter",
    "title": "Poisson Regression Examples",
    "section": "Deriving the MLE for the Poisson Rate Parameter",
    "text": "Deriving the MLE for the Poisson Rate Parameter\nRecall the log-likelihood function for a Poisson model with observations ( Y_1, , Y_n ):\n\\[\n\\ell(\\lambda) = -n\\lambda + \\left( \\sum_{i=1}^n Y_i \\right) \\log \\lambda - \\sum_{i=1}^n \\log(Y_i!)\n\\]\n\nStep 1: Differentiate the log-likelihood\n\\[\n\\frac{d\\ell}{d\\lambda} = -n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i\n\\]\n\n\nStep 2: Set the derivative equal to zero\n\\[\n-n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i = 0\n\\]\n\\[\n\\frac{1}{\\lambda} \\sum_{i=1}^n Y_i = n\n\\]\n\\[\n\\sum_{i=1}^n Y_i = n\\lambda\n\\]\n\n\nStep 3: Solve\n\\[\n\\hat{\\lambda}_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^n Y_i = \\bar{Y}\n\\]\nThus, the maximum likelihood estimator for ( ) is the sample mean of the data. This is consistent with the fact that the mean of a Poisson distribution is ( ).\nThe following code uses a numerical optimizer to find the value of 𝜆 that maximizes the log-likelihood function. This gives us the maximum likelihood estimate (MLE) for 𝜆 based on the observed data.\n\nfrom scipy.optimize import minimize_scalar\n\nneg_log_likelihood = lambda lambda_: -poisson_loglikelihood(lambda_, Y)\n\nresult = minimize_scalar(neg_log_likelihood, bounds=(0.01, 20), method='bounded')\n\nlambda_mle = result.x\nlambda_mle\n\nnp.float64(3.684666794827123)\n\n\n\n\nEstimation of Poisson Regression Model\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nThe following code updates the log-likelihood function to handle a Poisson regression model with covariates. Instead of using a constant 𝜆,it models 𝜆𝑖 as an exponential function of predictors 𝑋𝑖 and coefficients 𝛽. This allows the expected count to vary based on the characteristics of each observation.\n\ndef poisson_loglikelihood(beta, X, Y):\n    beta = np.asarray(beta)\n    eta = X @ beta\n    eta = np.clip(eta, -20, 20) \n    lam = np.exp(eta)\n    return np.sum(Y * eta - lam - gammaln(Y + 1))\n\nThe following code uses numerical optimization to estimate the 𝛽 coefficients in a Poisson regression model with covariates. It sets up the design matrix with an intercept, age, age squared, region dummies, and a customer indicator. After finding the maximum likelihood estimates, it uses the inverse of the Hessian matrix to calculate standard errors and summarizes the results in a table.\n\nfrom scipy.optimize import minimize\n\ndf['age_sq'] = df['age'] ** 2\nX_manual = pd.concat([\n    pd.Series(1, index=df.index, name='intercept'),\n    df[['age', 'age_sq', 'iscustomer']],\n    pd.get_dummies(df['region'], drop_first=True)\n], axis=1)\n\nX_matrix_manual = X_manual.astype(float).values\nY_vector = df['patents'].values\nvariable_names_manual = X_manual.columns\n\n\ndef neg_loglikelihood_manual(beta):\n    return -poisson_loglikelihood(beta, X_matrix_manual, Y_vector)\n\n\ninitial_beta = np.zeros(X_matrix_manual.shape[1])\nopt_result_manual = minimize(neg_loglikelihood_manual, initial_beta, method='BFGS')\n\nbeta_mle_manual = opt_result_manual.x\nhessian_inv_manual = opt_result_manual.hess_inv\nstandard_errors_manual = np.sqrt(np.diag(hessian_inv_manual))\n\nmanual_results_table = pd.DataFrame({\n    'Coefficient': beta_mle_manual,\n    'Std. Error': standard_errors_manual\n}, index=variable_names_manual)\n\nmanual_results_table.round(3)\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\n\n\n\n\nintercept\n-0.510\n0.193\n\n\nage\n0.149\n0.014\n\n\nage_sq\n-0.003\n0.000\n\n\niscustomer\n0.208\n0.033\n\n\nNortheast\n0.029\n0.047\n\n\nNorthwest\n-0.018\n0.057\n\n\nSouth\n0.057\n0.056\n\n\nSouthwest\n0.051\n0.050\n\n\n\n\n\n\n\nCheck results using Python sm.GLM() function.\n\nimport statsmodels.api as sm\nfrom IPython.display import display\n\ndf['age_sq'] = df['age'] ** 2\nregion_dummies = pd.get_dummies(df['region'], drop_first=True)  \nX = pd.concat([\n    pd.Series(1, index=df.index, name='intercept'),  \n    df[['age', 'age_sq', 'iscustomer']],\n    region_dummies\n], axis=1).astype(float) \n\npoisson_model = sm.GLM(Y_vector, X, family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\n\nglm_summary = poisson_results.summary2().tables[1].round(3)\ndisplay(glm_summary)\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\n\n\nintercept\n-0.509\n0.183\n-2.778\n0.005\n-0.868\n-0.150\n\n\nage\n0.149\n0.014\n10.716\n0.000\n0.121\n0.176\n\n\nage_sq\n-0.003\n0.000\n-11.513\n0.000\n-0.003\n-0.002\n\n\niscustomer\n0.208\n0.031\n6.719\n0.000\n0.147\n0.268\n\n\nNortheast\n0.029\n0.044\n0.669\n0.504\n-0.056\n0.115\n\n\nNorthwest\n-0.018\n0.054\n-0.327\n0.744\n-0.123\n0.088\n\n\nSouth\n0.057\n0.053\n1.074\n0.283\n-0.047\n0.160\n\n\nSouthwest\n0.051\n0.047\n1.072\n0.284\n-0.042\n0.143\n\n\n\n\n\n\n\nInterpret the results.\nThe Poisson regression results provide insights into how firm characteristics influence the number of patents awarded over a five-year period. The model includes predictors such as firm age, age squared (to capture non-linear effects), regional location, and whether the firm is a customer of Blueprinty.\nThe coefficient for firm age is positive and statistically significant, suggesting that older firms tend to receive more patents. However, the negative and significant coefficient for age squared indicates diminishing returns to age — the rate at which firms gain patents slows down as they get older. This reflects a concave (inverted-U) relationship: patent activity increases with experience but eventually plateaus or even declines.\nMost notably, being a customer of Blueprinty is associated with a higher patent success rate. The coefficient for the customer variable is approximately 0.208, which is statistically significant at conventional levels (p &lt; 0.001). This implies that, holding all else constant, Blueprinty customers are expected to file about 23% more patents than non-customers,exp(0.208)≈1.23. This provides quantitative support for Blueprinty’s marketing claim, although causality cannot be confirmed without randomized data.\nRegional effects appear to be minimal. None of the region dummy variables are statistically significant, indicating that once firm age and Blueprinty usage are accounted for, the location of the firm does not have a meaningful impact on patenting success.\nIn summary, the model suggests that Blueprinty’s software is positively associated with patenting activity, and that firm age plays a strong — but non-linear — role in patent output. Regional differences do not seem to contribute significantly to variation in patent counts among firms in this sample.\nThe following code estimates the effect of using Blueprinty’s software by comparing predicted patent counts for two scenarios: one where no firm is a customer (X_0), and one where all firms are customers (X_1). By calculating the average difference in predicted patents between these two scenarios, we get a clear, interpretable estimate of the software’s impact on patent success.\n\nX_0 = X.copy()\nX_1 = X.copy()\nX_0['iscustomer'] = 0\nX_1['iscustomer'] = 1\n\ny_pred_0 = poisson_results.predict(X_0)\ny_pred_1 = poisson_results.predict(X_1)\n\ndiff = y_pred_1 - y_pred_0\naverage_diff = np.mean(diff)\naverage_diff\n\nnp.float64(0.7927680710452896)\n\n\nThe average effect of using Blueprinty’s software, based on the model predictions, is approximately 0.79 additional patents per firm over the 5-year period.\nThis means that, all else being equal, firms using Blueprinty are predicted to have nearly one more awarded patent on average compared to if they had not used the software. This provides a more intuitive, interpretable measure of Blueprinty’s impact beyond the log-scale regression coefficient."
  },
  {
    "objectID": "blog/project2/index.html#airbnb-case-study",
    "href": "blog/project2/index.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\nThe following code performs an exploratory analysis of Airbnb data, assuming the number of reviews reflects the number of bookings. It begins by cleaning the data, dropping rows with missing values in key variables. Then, it fits a Poisson regression model to examine how listing features—such as price, room type, and review scores—relate to the number of reviews. The resulting model coefficients are used to interpret how each variable influences booking activity.\n\ndf_airbnb = pd.read_csv('airbnb.csv')\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nrelevant_cols = [\n    'number_of_reviews', 'bathrooms', 'bedrooms', 'price', 'days',\n    'review_scores_cleanliness', 'review_scores_location',\n    'review_scores_value', 'room_type', 'instant_bookable'\n]\ndf_clean = df_airbnb[relevant_cols].dropna()\n\ndf_clean['instant_bookable'] = df_clean['instant_bookable'].map({'t': 1, 'f': 0})\nroom_dummies = pd.get_dummies(df_clean['room_type'], drop_first=True)\ndf_clean = df_clean.drop(columns='room_type').join(room_dummies)\n\nsummary_stats = df_clean.describe()\n\nfig, axs = plt.subplots(2, 3, figsize=(15, 8))\nsns.histplot(df_clean['number_of_reviews'], bins=50, ax=axs[0, 0])\naxs[0, 0].set_title('Number of Reviews')\n\nsns.histplot(df_clean['price'], bins=50, ax=axs[0, 1])\naxs[0, 1].set_title('Price')\n\nsns.histplot(df_clean['days'], bins=50, ax=axs[0, 2])\naxs[0, 2].set_title('Days Listed')\n\nsns.histplot(df_clean['review_scores_cleanliness'], bins=10, ax=axs[1, 0])\naxs[1, 0].set_title('Cleanliness Score')\n\nsns.histplot(df_clean['review_scores_location'], bins=10, ax=axs[1, 1])\naxs[1, 1].set_title('Location Score')\n\nsns.histplot(df_clean['review_scores_value'], bins=10, ax=axs[1, 2])\naxs[1, 2].set_title('Value Score')\n\nplt.tight_layout()\nplt.show()\n\nplt.figure(figsize=(10, 6))\nsns.heatmap(df_clean.corr(), annot=True, fmt=\".2f\", cmap='coolwarm')\nplt.title(\"Correlation Matrix\")\nplt.show()\n\nsummary_stats\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnumber_of_reviews\nbathrooms\nbedrooms\nprice\ndays\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\ninstant_bookable\n\n\n\n\ncount\n30160.000000\n30160.000000\n30160.000000\n30160.000000\n30160.000000\n30160.000000\n30160.000000\n30160.000000\n30160.000000\n\n\nmean\n21.170889\n1.122132\n1.151459\n140.206863\n1139.711174\n9.201724\n9.415351\n9.333952\n0.196187\n\n\nstd\n32.007541\n0.384916\n0.699010\n188.392314\n1252.303675\n1.114261\n0.843185\n0.900472\n0.397118\n\n\nmin\n1.000000\n0.000000\n0.000000\n10.000000\n7.000000\n2.000000\n2.000000\n2.000000\n0.000000\n\n\n25%\n3.000000\n1.000000\n1.000000\n70.000000\n584.000000\n9.000000\n9.000000\n9.000000\n0.000000\n\n\n50%\n8.000000\n1.000000\n1.000000\n103.000000\n1041.000000\n10.000000\n10.000000\n10.000000\n0.000000\n\n\n75%\n26.000000\n1.000000\n1.000000\n169.000000\n1592.000000\n10.000000\n10.000000\n10.000000\n0.000000\n\n\nmax\n421.000000\n6.000000\n10.000000\n10000.000000\n42828.000000\n10.000000\n10.000000\n10.000000\n1.000000\n\n\n\n\n\n\n\n\n\nDistribution Insights:\nNumber of reviews is heavily right-skewed (most listings have few reviews, a few have hundreds).\nPrice has extreme outliers (max is $10,000), suggesting need for potential log transformation in future models.\nReview scores (cleanliness, location, value) are clustered near the top (mostly 8–10).\nDays listed ranges widely, up to ~42,000 days — possibly inflated by scrape or listing errors.\n\n\nCorrelations (with number of reviews):\ndays listed and instant_bookable show slight positive correlations (~0.1).\nprice, room_type, and review scores are weakly correlated with review count.\nStronger predictors likely interact or relate non-linearly, which a regression model can explore better.\n\nimport statsmodels.api as sm\nfrom IPython.display import display\n\nX_model = df_clean.drop(columns='number_of_reviews')\n\nX_model = pd.get_dummies(X_model, drop_first=True).astype(float)  \nX_model = sm.add_constant(X_model)\ny_model = df_clean['number_of_reviews']\n\npoisson_model = sm.GLM(y_model, X_model, family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\n\n\npoisson_summary = poisson_results.summary2().tables[1].round(3)\ndisplay(poisson_summary)\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\n\n\nconst\n3.498\n0.016\n217.396\n0.000\n3.467\n3.530\n\n\nbathrooms\n-0.118\n0.004\n-31.394\n0.000\n-0.125\n-0.110\n\n\nbedrooms\n0.074\n0.002\n37.197\n0.000\n0.070\n0.078\n\n\nprice\n-0.000\n0.000\n-2.151\n0.031\n-0.000\n-0.000\n\n\ndays\n0.000\n0.000\n129.755\n0.000\n0.000\n0.000\n\n\nreview_scores_cleanliness\n0.113\n0.001\n75.611\n0.000\n0.110\n0.116\n\n\nreview_scores_location\n-0.077\n0.002\n-47.796\n0.000\n-0.080\n-0.074\n\n\nreview_scores_value\n-0.091\n0.002\n-50.490\n0.000\n-0.095\n-0.088\n\n\ninstant_bookable\n0.346\n0.003\n119.666\n0.000\n0.340\n0.352\n\n\nPrivate room\n-0.011\n0.003\n-3.847\n0.000\n-0.016\n-0.005\n\n\nShared room\n-0.246\n0.009\n-28.578\n0.000\n-0.263\n-0.229\n\n\n\n\n\n\n\n\n\nInterpretation:\nListings with more bedrooms and longer availability (days) tend to receive more reviews.\nThe negative coefficient on bathrooms may reflect quirks in listing types (e.g., luxury places with fewer stays).\nPrice has a very small but statistically significant negative effect — likely due to higher prices deterring short-term bookings."
  },
  {
    "objectID": "blog/project3/index.html",
    "href": "blog/project3/index.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "Suppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "blog/project3/index.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "blog/project3/index.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "Suppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "blog/project3/index.html#simulate-conjoint-data",
    "href": "blog/project3/index.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed."
  },
  {
    "objectID": "blog/project3/index.html#preparing-the-data-for-estimation",
    "href": "blog/project3/index.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\n\nimport pandas as pd\ndf = pd.read_csv('conjoint_data.csv')\n\nbrand_dummies = pd.get_dummies(df['brand'], prefix='brand', drop_first=True, dtype=int)\nad_dummies = pd.get_dummies(df['ad'], prefix='ad', drop_first=True, dtype=int)\n\ndf_prep = pd.concat([\n    df[['resp', 'task', 'price', 'choice']],\n    brand_dummies,\n    ad_dummies\n], axis=1)\n\ndf_prep = df_prep.sort_values(['resp', 'task']).reset_index(drop=True)\n\ndf_prep.head(10)\n\n\n\n\n\n\n\n\nresp\ntask\nprice\nchoice\nbrand_N\nbrand_P\nad_Yes\n\n\n\n\n0\n1\n1\n28\n1\n1\n0\n1\n\n\n1\n1\n1\n16\n0\n0\n0\n1\n\n\n2\n1\n1\n16\n0\n0\n1\n1\n\n\n3\n1\n2\n32\n0\n1\n0\n1\n\n\n4\n1\n2\n16\n1\n0\n1\n1\n\n\n5\n1\n2\n24\n0\n1\n0\n1\n\n\n6\n1\n3\n8\n0\n0\n1\n0\n\n\n7\n1\n3\n24\n1\n0\n0\n1\n\n\n8\n1\n3\n24\n0\n1\n0\n0\n\n\n9\n1\n4\n28\n0\n0\n1\n0"
  },
  {
    "objectID": "blog/project3/index.html#estimation-via-maximum-likelihood",
    "href": "blog/project3/index.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\n_Code up the log-likelihood function, to find the MLEs for the 4 parameters (\\(\\beta_\\text{netflix}\\), \\(\\beta_\\text{prime}\\), \\(\\beta_\\text{ads}\\), \\(\\beta_\\text{price}\\)), as well as their standard errors (from the Hessian). For each parameter construct a 95% confidence interval.__\n\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.special import logsumexp\n\n# Extract X matrix and y vector\nX = df_prep[['brand_N','brand_P','ad_Yes','price']].values\ny = df_prep['choice'].values\n\n# Define number of tasks per respondent and alternatives per task\nn_alts = 3\n\n# Define the negative log-likelihood function for the MNL model\ndef neg_log_likelihood(beta, X, y, n_alts):\n    beta = np.array(beta)\n    utilities = X @ beta\n    utilities = utilities.reshape(-1, n_alts)\n    log_probs = utilities - logsumexp(utilities, axis=1, keepdims=True)\n    log_likelihood = np.sum(y.reshape(-1, n_alts) * log_probs)\n    return -log_likelihood\n\n# Initial guess for beta\nbeta_init = np.zeros(X.shape[1])\n\n# Minimize the negative log-likelihood\nresult = minimize(neg_log_likelihood, beta_init, args=(X, y, n_alts), method='BFGS')\n\n# Extract estimates and compute standard errors from the Hessian\nbeta_mle = result.x\nhessian_inv = result.hess_inv\nse = np.sqrt(np.diag(hessian_inv))\nz = 1.96  # for 95% CI\n\n# Confidence intervals\nci_lower = beta_mle - z * se\nci_upper = beta_mle + z * se\n\n# Compile results into a dataframe\nmle_results = pd.DataFrame({\n    \"Parameter\": [\"beta_netflix\", \"beta_prime\", \"beta_ads\", \"beta_price\"],\n    \"Estimate\": beta_mle,\n    \"Std. Error\": se,\n    \"95% CI Lower\": ci_lower,\n    \"95% CI Upper\": ci_upper\n})\n\nmle_results.round(3)\n\n\n\n\n\n\n\n\nParameter\nEstimate\nStd. Error\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\nbeta_netflix\n0.941\n0.114\n0.717\n1.165\n\n\n1\nbeta_prime\n0.502\n0.121\n0.265\n0.738\n\n\n2\nbeta_ads\n-0.732\n0.089\n-0.906\n-0.558\n\n\n3\nbeta_price\n-0.099\n0.006\n-0.112\n-0.087"
  },
  {
    "objectID": "blog/project3/index.html#estimation-via-bayesian-methods",
    "href": "blog/project3/index.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\ncode up a metropolis-hasting MCMC sampler of the posterior distribution. Take 11,000 steps and throw away the first 1,000, retaining the subsequent 10,000.\n\nimport numpy.random as npr\n\n# Define log prior function\ndef log_prior(beta):\n    # Normal priors: N(0,5) for first 3 betas, N(0,1) for price\n    logp = -0.5 * (beta[0]**2 + beta[1]**2 + beta[2]**2) / 25  # variance = 5^2 = 25\n    logp += -0.5 * (beta[3]**2)  # variance = 1\n    return logp\n\n# Log posterior function\ndef log_posterior(beta, X, y, n_alts):\n    return -neg_log_likelihood(beta, X, y, n_alts) + log_prior(beta)\n\n# Metropolis-Hastings MCMC sampler\ndef metropolis_hastings(log_post, initial_beta, X, y, n_alts, n_iter=11000, proposal_sd=[0.05, 0.05, 0.05, 0.005]):\n    n_params = len(initial_beta)\n    samples = np.zeros((n_iter, n_params))\n    beta_current = initial_beta\n    log_post_current = log_post(beta_current, X, y, n_alts)\n    \n    for t in range(n_iter):\n        # Propose new beta from normal distributions\n        beta_proposal = beta_current + npr.normal(0, proposal_sd)\n        log_post_proposal = log_post(beta_proposal, X, y, n_alts)\n        \n        # Acceptance probability\n        accept_ratio = np.exp(log_post_proposal - log_post_current)\n        \n        if np.random.rand() &lt; accept_ratio:\n            beta_current = beta_proposal\n            log_post_current = log_post_proposal\n        \n        samples[t] = beta_current\n    \n    return samples\n\n# Run the MCMC sampler\ninitial_beta = np.zeros(4)\nsamples = metropolis_hastings(log_posterior, initial_beta, X, y, n_alts)\n\n# Discard the first 1,000 samples (burn-in)\nposterior_samples = samples[1000:]\n\n# Summarize posterior samples\nposterior_summary = pd.DataFrame({\n    \"Parameter\": [\"beta_netflix\", \"beta_prime\", \"beta_ads\", \"beta_price\"],\n    \"Mean\": posterior_samples.mean(axis=0),\n    \"Std. Dev.\": posterior_samples.std(axis=0),\n    \"2.5%\": np.percentile(posterior_samples, 2.5, axis=0),\n    \"97.5%\": np.percentile(posterior_samples, 97.5, axis=0)\n})\nposterior_summary.round(3)\n\n\n\n\n\n\n\n\nParameter\nMean\nStd. Dev.\n2.5%\n97.5%\n\n\n\n\n0\nbeta_netflix\n0.949\n0.110\n0.741\n1.169\n\n\n1\nbeta_prime\n0.506\n0.107\n0.305\n0.716\n\n\n2\nbeta_ads\n-0.735\n0.092\n-0.913\n-0.558\n\n\n3\nbeta_price\n-0.100\n0.007\n-0.113\n-0.087\n\n\n\n\n\n\n\nfor one of the 4 parameters, I will show the trace plot of the algorithm, as well as the histogram of the posterior distribution.\n\nimport matplotlib.pyplot as plt\n\n# Choose one parameter to visualize: beta_netflix (index 0)\ntrace_samples = posterior_samples[:, 0]\n\n# Plot trace and histogram\nplt.figure(figsize=(12, 5))\n\n# Trace plot\nplt.subplot(1, 2, 1)\nplt.plot(trace_samples, lw=0.5)\nplt.title(\"Trace Plot for beta_netflix\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Value\")\n\n# Histogram\nplt.subplot(1, 2, 2)\nplt.hist(trace_samples, bins=40, density=True, edgecolor='k')\nplt.title(\"Posterior Distribution for beta_netflix\")\nplt.xlabel(\"Value\")\nplt.ylabel(\"Density\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nNext I will report the 4 posterior means, standard deviations, and 95% credible intervals and compare them to the results from the Maximum Likelihood approach.\n\ncomparison = mle_results.copy()\ncomparison.columns = [\"Parameter\", \"MLE Estimate\", \"MLE Std. Error\", \"MLE 95% CI Lower\", \"MLE 95% CI Upper\"]\n\ncomparison[\"Bayes Mean\"] = posterior_summary[\"Mean\"]\ncomparison[\"Bayes Std. Dev.\"] = posterior_summary[\"Std. Dev.\"]\ncomparison[\"Bayes 95% CI Lower\"] = posterior_summary[\"2.5%\"]\ncomparison[\"Bayes 95% CI Upper\"] = posterior_summary[\"97.5%\"]\n\ncomparison.round(3)\n\n\n\n\n\n\n\n\nParameter\nMLE Estimate\nMLE Std. Error\nMLE 95% CI Lower\nMLE 95% CI Upper\nBayes Mean\nBayes Std. Dev.\nBayes 95% CI Lower\nBayes 95% CI Upper\n\n\n\n\n0\nbeta_netflix\n0.941\n0.114\n0.717\n1.165\n0.949\n0.110\n0.741\n1.169\n\n\n1\nbeta_prime\n0.502\n0.121\n0.265\n0.738\n0.506\n0.107\n0.305\n0.716\n\n\n2\nbeta_ads\n-0.732\n0.089\n-0.906\n-0.558\n-0.735\n0.092\n-0.913\n-0.558\n\n\n3\nbeta_price\n-0.099\n0.006\n-0.112\n-0.087\n-0.100\n0.007\n-0.113\n-0.087"
  },
  {
    "objectID": "blog/project3/index.html#discussion",
    "href": "blog/project3/index.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\nThe comparison of Maximum Likelihood Estimation (MLE) and Bayesian posterior summaries reveals close alignment in the estimated parameters, indicating robustness of the model across estimation methods. With this in mind, we interpret the parameter estimates as if they had been obtained from real (non-simulated) data and discuss the potential implications for modeling preference heterogeneity.\n\nInterpretation of Parameter Estimates\nIf these parameter estimates were derived from actual consumer data, they would yield valuable insights into preferences over key streaming service attributes, including brand, advertising, and price.\n\nRelative Brand Preferences\nThe estimate for \\(\\beta_{\\text{Netflix}}\\) is larger than that for \\(\\beta_{\\text{Prime}}\\), both of which are positive relative to the reference category, Hulu. This suggests that, all else equal, consumers derive the highest utility from Netflix, followed by Amazon Prime, and then Hulu. In practical terms, this implies a clear brand preference hierarchy that aligns with Netflix’s market leadership and perceived content quality.\n\n\nEffect of Advertisements\nThe negative coefficient on the ads variable indicates that advertisement inclusion reduces the utility of a streaming service. This result is consistent with consumer expectations and supports the idea that ad-free plans are generally preferred. For ad-supported models to be attractive, they may need to offer lower pricing or other incentives.\n\n\nPrice Sensitivity\nThe negative sign of the \\(\\beta_{\\text{price}}\\) coefficient aligns with standard economic theory: higher prices reduce the probability that a product will be chosen. The estimated magnitude suggests that each additional dollar in monthly subscription cost reduces utility by approximately 0.1 units, providing a quantifiable measure of consumers’ price sensitivity.\n\n\nSummary\nOverall, the parameter estimates are intuitive and align with expectations. They reflect strong brand preferences, disutility from advertisements, and rational sensitivity to pricing—factors critical for effective product design and positioning in the streaming services market.\n\n\n\n\nToward More Realistic Preference Modeling\nWhile the fixed-parameter MNL model offers valuable insights, it assumes that all consumers share the same preferences. In real-world applications, this assumption is overly restrictive. To account for heterogeneity in preferences, a more flexible model is required.\n\n\nExtension to a Hierarchical (Random-Parameter) Model\nA more realistic framework for analyzing conjoint data is the hierarchical multinomial logit model (also known as the mixed logit model). This model relaxes the assumption of homogeneous preferences by allowing each individual to have their own set of preference parameters.\n\n1. Simulating Data\nTo simulate data under a hierarchical structure:\n\nDefine a distribution over individual-level parameters, such as \\(\\beta_i \\sim \\mathcal{N}(\\mu, \\Sigma)\\), instead of assuming fixed effects.\nFor each individual \\(i\\), draw a personalized vector of preferences \\(\\beta_i\\) from this distribution.\nUse these respondent-specific parameters to simulate choices, capturing variation in how different individuals value features like brand, ads, and price.\n\nThis process generates richer data that reflects the natural diversity in consumer preferences.\n\n\n2. Estimation Adjustments\nTo estimate the parameters of a hierarchical MNL model, we must integrate over the distribution of individual-level betas, which introduces computational complexity:\n\nThe resulting likelihood function contains an integral over the unobserved heterogeneity, which typically lacks a closed-form solution.\nTwo common approaches to estimation are:\n\nHierarchical Bayesian estimation, which uses MCMC to jointly estimate the individual-level parameters and population-level hyperparameters (\\(\\mu\\), \\(\\Sigma\\)).\nMaximum Simulated Likelihood (MSL), which approximates the integral by averaging over simulated draws from the assumed distribution of \\(\\beta_i\\).\n\n\nThese methods allow us to recover both:\n\nThe population distribution of preferences, which informs aggregate-level strategies,\nAnd individual-level estimates, which can guide personalization and segmentation.\n\n\n\nSummary\nIncorporating a hierarchical structure into the MNL model significantly enhances its realism and interpretability. By explicitly modeling variation across individuals, the hierarchical approach supports more accurate inference and decision-making. This extension is essential for analyzing real-world conjoint data, where preference heterogeneity is the norm rather than the exception."
  }
]