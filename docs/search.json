[
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Projects",
    "section": "",
    "text": "A Replication of Karlan and List (2007)\n\n\n\n\n\n\nYiwei(Jerry) Huang\n\n\nApr 23, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Yiwei(Jerry) Huang",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "hw1_questions.html",
    "href": "hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "hw1_questions.html#introduction",
    "href": "hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "hw1_questions.html#data",
    "href": "hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\ntodo: Read the data into R/Python and describe the data\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pyrsm as rsm\ndf = pd.read_stata(r\"c:\\Users\\13816\\Desktop\\karlan_list_2007.dta\")\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\ntodo: test a few variables other than the key outcome variables (for example, test months since last donation) to see if the treatment and control groups are statistically significantly different at the 95% confidence level. Do each as a t-test and separately as a linear regression, and confirm you get the exact same results from both methods. When doing a t-test, use the formula in the class slides. When doing the linear regression, regress for example mrm2 on treatment and look at the estimated coefficient on the treatment variable. It might be helpful to compare parts of your analysis to Table 1 in the paper. Be sure to comment on your results (hint: why is Table 1 included in the paper).\n\ntreat = df[df['treatment'] == 1]['mrm2']\ncontrol = df[df['treatment'] == 0]['mrm2']\n\nmean_treat = treat.mean()\nmean_control = control.mean()\nvar_treat = treat.var(ddof=1)\nvar_control = control.var(ddof=1)\nn_treat = treat.count()\nn_control = control.count()\n\nnumerator = mean_treat - mean_control\ndenominator = np.sqrt(var_treat / n_treat + var_control / n_control)\nt_stat = numerator / denominator\n\nprint(\"Mean Treatment:\", round(mean_treat,3))\nprint(\"Std Treatment:\", round(np.sqrt(var_treat),3))\n\nprint(\"Mean Control:\", round(mean_control,3))\nprint(\"Std Control:\", round(np.sqrt(var_control),3))\nprint(\"Overall Mean:\", round(df['mrm2'].mean(),3))\nprint(\"Overall Std:\", round(df['mrm2'].std(),3))\nprint(\"t test:\", t_stat)\n\nMean Treatment: 13.012\nStd Treatment: 12.086\nMean Control: 12.998\nStd Control: 12.074\nOverall Mean: 13.007\nOverall Std: 12.081\nt test: 0.1195315522817725\n\n\n\nmodel = rsm.model.regress(data = df, rvar = 'mrm2', evar = 'treatment')\nmodel.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : mrm2\nExplanatory variables: treatment\nNull hyp.: the effect of x on mrm2 is zero\nAlt. hyp.: the effect of x on mrm2 is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept       12.998      0.094  138.979  &lt; .001  ***\ntreatment        0.014      0.115    0.119   0.905     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.0\nF-statistic: 0.014 df(1, 50080), p.value 0.905\nNr obs: 50,082 (1 obs. dropped)\n\n\nThe manual t-test and linear regression results for mrm2 (months since last donation) show no statistically significant difference between the treatment and control groups (t ≈ 0.12, p ≈ 0.905), indicating that this key pre-treatment variable is well balanced. This supports the validity of the random assignment and confirms that the experimental groups are comparable before the intervention. Table 1 is included in the paper to show that randomization produced balanced groups across a wide range of covariates, ensuring that any observed differences in outcomes can be attributed to the treatment rather than pre-existing differences."
  },
  {
    "objectID": "hw1_questions.html#experimental-results",
    "href": "hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\ntodo: make a barplot with two bars. Each bar is the proportion of people who donated. One bar for treatment and one bar for control.\n\nprop_donated = df.groupby('treatment')['gave'].mean().reset_index()\nprop_donated['group'] = prop_donated['treatment'].map({0: 'Control', 1: 'Treatment'})\n\nsns.barplot(data=prop_donated, x='group', y='gave')\nplt.ylabel(\"Proportion Who Donated\")\nplt.title(\"Donation Rate by Group\")\nplt.show()\n\n\n\n\n\n\n\n\ntodo: run a t-test between the treatment and control groups on the binary outcome of whether any charitable donation was made. Also run a bivariate linear regression that demonstrates the same finding. (It may help to confirm your calculations match Table 2a Panel A.) Report your statistical results and interpret them in the context of the experiment (e.g., if you found a difference with a small p-value or that was statistically significant at some threshold, what have you learned about human behavior? Use mostly English words, not numbers or stats, to explain your finding.)\n\ntreat = df[df['treatment'] == 1]['gave']\ncontrol = df[df['treatment'] == 0]['gave']\n\nmean_diff = treat.mean() - control.mean()\nvar_treat = treat.var()\nvar_control = control.var()\nn_treat = len(treat)\nn_control = len(control)\n\nt_stat = mean_diff / np.sqrt(var_treat / n_treat + var_control / n_control)\nprint(\"T-statistic:\", t_stat)\n\nT-statistic: 3.2094621908279835\n\n\n\nmodel = rsm.model.regress(data = df, rvar = 'gave', evar = 'treatment')\nmodel.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : gave\nExplanatory variables: treatment\nNull hyp.: the effect of x on gave is zero\nAlt. hyp.: the effect of x on gave is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.018      0.001   16.225  &lt; .001  ***\ntreatment        0.004      0.001    3.101   0.002   **\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: 0.0\nF-statistic: 9.618 df(1, 50081), p.value 0.002\nNr obs: 50,083\n\n\nThe statistical analysis shows that individuals who received the treatment — a matching donation offer — were significantly more likely to make a charitable contribution compared to those who received the control letter. Both the t-test and linear regression indicate that this difference is unlikely to be due to chance. This suggests that the presence of a matching grant acts as a powerful motivator, nudging people toward taking action. In the context of human behavior, this reveals that even a subtle change in framing — like knowing one’s gift will be matched — can meaningfully increase the likelihood of giving. It highlights how social cues or perceived amplification of impact can influence decision-making in charitable contexts.\ntodo: run a probit regression where the outcome variable is whether any charitable donation was made and the explanatory variable is assignment to treatment or control. Confirm that your results replicate Table 3 column 1 in the paper.\n\nfrom statsmodels.discrete.discrete_model import Probit\n\nprobit_model = Probit.from_formula('gave ~ treatment', data=df)\nprobit_result = probit_model.fit()\nprint(probit_result.summary())\nmfx = probit_result.get_margeff()\nprint(mfx.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                   gave   No. Observations:                50083\nModel:                         Probit   Df Residuals:                    50081\nMethod:                           MLE   Df Model:                            1\nDate:                Sun, 20 Apr 2025   Pseudo R-squ.:               0.0009783\nTime:                        15:17:21   Log-Likelihood:                -5030.5\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                  0.001696\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -2.1001      0.023    -90.073      0.000      -2.146      -2.054\ntreatment      0.0868      0.028      3.113      0.002       0.032       0.141\n==============================================================================\n       Probit Marginal Effects       \n=====================================\nDep. Variable:                   gave\nMethod:                          dydx\nAt:                           overall\n==============================================================================\n                dy/dx    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\ntreatment      0.0043      0.001      3.104      0.002       0.002       0.007\n==============================================================================\n\n\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\ntodo: Use a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not. For example, does the 2:1 match rate lead increase the likelihood that someone donates as compared to the 1:1 match rate? Do your results support the “figures suggest” comment the authors make on page 8?\n\n# t-test between 2:1 and 1:1 ratios\ngave_1_1 = df[df['ratio'] == 1]['gave']\ngave_2_1 = df[df['ratio'] == 2]['gave']\n\nfrom scipy.stats import ttest_ind\nt_stat, p_val = ttest_ind(gave_2_1, gave_1_1, equal_var=False)\n\nprint(\"2:1 vs 1:1 Match Rate:\")\nprint(f\"T-stat: {t_stat:.3f}, P-value: {p_val:.3f}\")\n\n2:1 vs 1:1 Match Rate:\nT-stat: 0.965, P-value: 0.335\n\n\n\n# t-test between 3:1 and 1:1 ratios\ngave_3_1 = df[df['ratio'] == 3]['gave']\ngave_1_1 = df[df['ratio'] == 1]['gave']\n\nt_stat, p_val = ttest_ind(gave_3_1, gave_1_1, equal_var=False)\nprint(\"3:1 vs 1:1 Match Rate:\")\nprint(f\"T-stat: {t_stat:.3f}, P-value: {p_val:.3f}\")\n\n3:1 vs 1:1 Match Rate:\nT-stat: 1.015, P-value: 0.310\n\n\n\n# t-test between 3:1 and 2:1 ratios\ngave_3_1 = df[df['ratio'] == 3]['gave']\ngave_2_1 = df[df['ratio'] == 2]['gave']\n\nt_stat, p_val = ttest_ind(gave_3_1, gave_2_1, equal_var=False)\nprint(\"3:1 vs 2:1 Match Rate:\")\nprint(f\"T-stat: {t_stat:.3f}, P-value: {p_val:.3f}\")\n\n3:1 vs 2:1 Match Rate:\nT-stat: 0.050, P-value: 0.960\n\n\nMy results clearly support the authors’ conclusion: while higher match ratios may show slightly higher donation rates numerically, those differences are not statistically significant, and therefore do not provide strong evidence that larger match sizes are more effective than 1:1 matches.\ntodo: Assess the same issue using a regression. Specifically, create the variable ratio1 then regress gave on ratio1, ratio2, and ratio3 (or alternatively, regress gave on the categorical variable ratio). Interpret the coefficients and their statistical precision.\n\nmodel = rsm.model.regress(data = df, rvar = 'gave', evar = 'ratio')\nmodel.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : gave\nExplanatory variables: ratio\nNull hyp.: the effect of x on gave is zero\nAlt. hyp.: the effect of x on gave is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.018      0.001   16.225  &lt; .001  ***\nratio[1]         0.003      0.002    1.661   0.097    .\nratio[2]         0.005      0.002    2.744   0.006   **\nratio[3]         0.005      0.002    2.802   0.005   **\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: 0.0\nF-statistic: 3.665 df(3, 50079), p.value 0.012\nNr obs: 50,083\n\n\nRegression results show that all three match ratios (1:1, 2:1, 3:1) are associated with slightly higher donation rates compared to the control group, but the differences are small. The coefficient for ratio[1] (1:1 match) is 0.003 and marginally significant (p = 0.097), while the coefficients for ratio[2] (2:1) and ratio[3] (3:1) are both 0.005 and statistically significant at the 1% level (p = 0.006 and p = 0.005). However, the differences between them are not large in magnitude — all are within 0.002 of each other — suggesting that although offering any match tends to increase donations, there is little evidence that larger match sizes lead to proportionally greater increases. This supports the conclusion that the presence of a match matters more than its generosity, and that people are generally responsive to the signal of support, not necessarily the size of the match.\ntodo: Calculate the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios. Do this directly from the data, and do it by computing the differences in the fitted coefficients of the previous regression. what do you conclude regarding the effectiveness of different sizes of matched donations?\n\np_1_1 = df[df['ratio'] == 1]['gave'].mean()\np_2_1 = df[df['ratio'] == 2]['gave'].mean()\np_3_1 = df[df['ratio'] == 3]['gave'].mean()\n\nprint(\"Response rates:\")\nprint(f\"1:1 = {p_1_1*100:.3f}%, 2:1 = {p_2_1*100:.3f}%, 3:1 = {p_3_1*100:.3f}%\")\n\nResponse rates:\n1:1 = 2.075%, 2:1 = 2.263%, 3:1 = 2.273%\n\n\nDifferences (From Regression Coefficients):\n2:1 − 1:1 = 0.005 − 0.003 = 0.002\n3:1 − 2:1 = 0.005 − 0.005 = 0.000\nBoth the raw response rates and regression coefficients indicate that increasing the match from 1:1 to 2:1 produces a small increase in the probability of donating (about 0.2 percentage points), while moving from 2:1 to 3:1 shows virtually no change. These differences are very modest in size, and the lack of improvement from 2:1 to 3:1 suggests diminishing returns to increasing match generosity. The findings reinforce the conclusion that offering any match increases donations, but offering a larger match ratio does not lead to proportionally greater giving. The signal of a match itself may be more powerful than the actual amount matched.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\ntodo: Calculate a t-test or run a bivariate linear regression of the donation amount on the treatment status. What do we learn from doing this analysis?\n\nmodel = rsm.model.regress(data = df, rvar = 'amount', evar = 'treatment')\nmodel.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : amount\nExplanatory variables: treatment\nNull hyp.: the effect of x on amount is zero\nAlt. hyp.: the effect of x on amount is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.813      0.067   12.063  &lt; .001  ***\ntreatment        0.154      0.083    1.861   0.063    .\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: 0.0\nF-statistic: 3.461 df(1, 50081), p.value 0.063\nNr obs: 50,083\n\n\nFrom this analysis, we learn that individuals who received the matching grant treatment donated slightly more on average than those in the control group, but the difference is only marginally statistically significant (p = 0.063). The treatment effect estimate suggests that the treatment group gave about 15 cents more per person than the control group, which is a small increase. This result implies that the primary effect of the matching grant is likely driven by increasing the number of people who give, rather than substantially increasing the donation amounts of those who would already have donated. It highlights that while the match offer motivates more people to donate, it doesn’t strongly influence how much they give on average.\ntodo: now limit the data to just people who made a donation and repeat the previous analysis. This regression allows you to analyze how much respondents donate conditional on donating some positive amount. Interpret the regression coefficients – what did we learn? Does the treatment coefficient have a causal interpretation?\n\nmodel = rsm.model.regress(data = df[df['gave'] == 1], rvar = 'amount', evar = 'treatment')\nmodel.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : amount\nExplanatory variables: treatment\nNull hyp.: the effect of x on amount is zero\nAlt. hyp.: the effect of x on amount is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept       45.540      2.423   18.792  &lt; .001  ***\ntreatment       -1.668      2.872   -0.581   0.561     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.001\nF-statistic: 0.337 df(1, 1032), p.value 0.561\nNr obs: 1,034\n\n\nThe regression results show that among individuals who made a donation, the treatment group gave slightly less on average than the control group, but this difference (−1.67) is not statistically significant (p = 0.561). This tells us that the matching grant treatment did not affect the amount given by those who chose to donate. In other words, the presence of a match increased the number of people who gave, but not how much they gave once they decided to give.\nAs for causal interpretation: since this regression is limited only to people who donated, it suffers from selection bias — treatment may have influenced who donated, and the group of donors in treatment and control may differ in unobserved ways. Therefore, the treatment coefficient cannot be interpreted causally in this conditional regression. Only the earlier regression using the full sample (which preserves random assignment) supports a causal interpretation of treatment effects.\ntodo: Make two plot: one for the treatment group and one for the control. Each plot should be a histogram of the donation amounts only among people who donated. Add a red vertical bar or some other annotation to indicate the sample average for each plot.\n\ndonors = df[df['gave'] == 1]\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\nsns.histplot(donors[donors['treatment'] == 0]['amount'], bins=30, ax=axes[0], color='skyblue')\naxes[0].axvline(donors[donors['treatment'] == 0]['amount'].mean(), color='red', linestyle='--')\naxes[0].set_title('Control Group')\naxes[0].set_xlabel('Donation Amount')\n\nsns.histplot(donors[donors['treatment'] == 1]['amount'], bins=30, ax=axes[1], color='lightgreen')\naxes[1].axvline(donors[donors['treatment'] == 1]['amount'].mean(), color='red', linestyle='--')\naxes[1].set_title('Treatment Group')\naxes[1].set_xlabel('Donation Amount')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "hw1_questions.html#simulation-experiment",
    "href": "hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nto do: Make a plot like those on slide 43 from our first class and explain the plot to the reader. To do this, you will simulate 100,00 draws from the control distribution and 10,000 draws from the treatment distribution. You’ll then calculate a vector of 10,000 differences, and then you’ll plot the cumulative average of that vector of differences. Comment on whether the cumulative average approaches the true difference in means.\n\nn = 10000\np_control = 0.018\np_treatment = 0.022\ntrue_diff = p_treatment - p_control \n\ncontrol = np.random.binomial(1, p_control, n)\ntreatment = np.random.binomial(1, p_treatment, n)\n\ndiffs = treatment - control \n\ncumulative_avg = np.cumsum(diffs) / np.arange(1, n + 1)\n\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_avg, label='Cumulative Average Difference')\nplt.axhline(true_diff, color='red', linestyle='--', label='True Difference = 0.004')\nplt.title('Law of Large Numbers: Cumulative Average of Treatment - Control')\nplt.xlabel('Number of Simulated Pairs')\nplt.ylabel('Average Difference')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe cumulative average of the difference in donation outcomes between the treatment and control groups begins with high variability but quickly stabilizes as more simulated pairs are added. By around 1,000–2,000 iterations, the average difference converges very closely to the true population difference of 0.004, shown by the red dashed line. This confirms that as the sample size increases, the sample average becomes a reliable and consistent estimator of the population average. In other words, the plot clearly shows that the cumulative average approaches the true difference in means, which is exactly what the Law of Large Numbers predicts.\n\n\nCentral Limit Theorem\nto do: Make 4 histograms like those on slide 44 from our first class at sample sizes 50, 200, 500, and 1000 and explain these plots to the reader. To do this for a sample size of e.g. 50, take 50 draws from each of the control and treatment distributions, and calculate the average difference between those draws. Then repeat that process 999 more times so that you have 1000 averages. Plot the histogram of those averages. Comment on whether zero is in the “middle” of the distribution or whether it’s in the “tail.”\n\np_control = 0.018\np_treatment = 0.022\ntrue_diff = p_treatment - p_control\nn_sim = 1000\nsample_sizes = [50, 200, 500, 1000]\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\naxes = axes.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    avg_diffs = []\n\n    for _ in range(n_sim):\n        control_sample = np.random.binomial(1, p_control, n)\n        treatment_sample = np.random.binomial(1, p_treatment, n)\n        diff = treatment_sample.mean() - control_sample.mean()\n        avg_diffs.append(diff)\n    \n    sns.histplot(avg_diffs, bins=30, kde=True, stat=\"frequency\", color=\"blue\", ax=axes[i])\n    axes[i].axvline(0, color='red', linestyle='--', linewidth=2)\n    axes[i].set_title(f\"Sample size = {n}\", fontsize=14)\n    axes[i].set_xlabel(\"Mean Difference (Treatment - Control)\")\n    axes[i].set_ylabel(\"Frequency\")\n\nplt.suptitle(\"Central Limit Theorem Simulation\", fontsize=16, y=1.02)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nIn the histogram for a sample size of 50, the distribution is wide and irregular, and zero appears near the center — indicating that at this small sample size, it’s still quite common to observe no difference between treatment and control groups due to random variation. As the sample size increases to 200 and 500, the distributions become more symmetric and bell-shaped, with zero gradually shifting away from the peak. By the time we reach a sample size of 1000, the distribution is tightly centered around a positive value, and zero clearly lies in the tail of the distribution. This shift demonstrates how larger samples improve our ability to detect even small differences and reduce the likelihood of falsely concluding that there’s no effect when one actually exists. In short, as sample size increases, zero moves from the middle toward the tails, confirming the predictions of the Central Limit Theorem and the increasing statistical power of larger samples."
  },
  {
    "objectID": "blog/project1/hw1_questions.html",
    "href": "blog/project1/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/project1/hw1_questions.html#introduction",
    "href": "blog/project1/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/project1/hw1_questions.html#data",
    "href": "blog/project1/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\ntodo: Read the data into R/Python and describe the data\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pyrsm as rsm\ndf = pd.read_stata(r\"karlan_list_2007.dta\")\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\ntodo: test a few variables other than the key outcome variables (for example, test months since last donation) to see if the treatment and control groups are statistically significantly different at the 95% confidence level. Do each as a t-test and separately as a linear regression, and confirm you get the exact same results from both methods. When doing a t-test, use the formula in the class slides. When doing the linear regression, regress for example mrm2 on treatment and look at the estimated coefficient on the treatment variable. It might be helpful to compare parts of your analysis to Table 1 in the paper. Be sure to comment on your results (hint: why is Table 1 included in the paper).\n\ntreat = df[df['treatment'] == 1]['mrm2']\ncontrol = df[df['treatment'] == 0]['mrm2']\n\nmean_treat = treat.mean()\nmean_control = control.mean()\nvar_treat = treat.var(ddof=1)\nvar_control = control.var(ddof=1)\nn_treat = treat.count()\nn_control = control.count()\n\nnumerator = mean_treat - mean_control\ndenominator = np.sqrt(var_treat / n_treat + var_control / n_control)\nt_stat = numerator / denominator\n\nprint(\"Mean Treatment:\", round(mean_treat,3))\nprint(\"Std Treatment:\", round(np.sqrt(var_treat),3))\n\nprint(\"Mean Control:\", round(mean_control,3))\nprint(\"Std Control:\", round(np.sqrt(var_control),3))\nprint(\"Overall Mean:\", round(df['mrm2'].mean(),3))\nprint(\"Overall Std:\", round(df['mrm2'].std(),3))\nprint(\"t test:\", t_stat)\n\nMean Treatment: 13.012\nStd Treatment: 12.086\nMean Control: 12.998\nStd Control: 12.074\nOverall Mean: 13.007\nOverall Std: 12.081\nt test: 0.1195315522817725\n\n\n\nmodel = rsm.model.regress(data = df, rvar = 'mrm2', evar = 'treatment')\nmodel.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : mrm2\nExplanatory variables: treatment\nNull hyp.: the effect of x on mrm2 is zero\nAlt. hyp.: the effect of x on mrm2 is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept       12.998      0.094  138.979  &lt; .001  ***\ntreatment        0.014      0.115    0.119   0.905     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.0\nF-statistic: 0.014 df(1, 50080), p.value 0.905\nNr obs: 50,082 (1 obs. dropped)\n\n\nThe manual t-test and linear regression results for mrm2 (months since last donation) show no statistically significant difference between the treatment and control groups (t ≈ 0.12, p ≈ 0.905), indicating that this key pre-treatment variable is well balanced. This supports the validity of the random assignment and confirms that the experimental groups are comparable before the intervention. Table 1 is included in the paper to show that randomization produced balanced groups across a wide range of covariates, ensuring that any observed differences in outcomes can be attributed to the treatment rather than pre-existing differences."
  },
  {
    "objectID": "blog/project1/hw1_questions.html#experimental-results",
    "href": "blog/project1/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\ntodo: make a barplot with two bars. Each bar is the proportion of people who donated. One bar for treatment and one bar for control.\n\nprop_donated = df.groupby('treatment')['gave'].mean().reset_index()\nprop_donated['group'] = prop_donated['treatment'].map({0: 'Control', 1: 'Treatment'})\n\nsns.barplot(data=prop_donated, x='group', y='gave')\nplt.ylabel(\"Proportion Who Donated\")\nplt.title(\"Donation Rate by Group\")\nplt.show()\n\n\n\n\n\n\n\n\ntodo: run a t-test between the treatment and control groups on the binary outcome of whether any charitable donation was made. Also run a bivariate linear regression that demonstrates the same finding. (It may help to confirm your calculations match Table 2a Panel A.) Report your statistical results and interpret them in the context of the experiment (e.g., if you found a difference with a small p-value or that was statistically significant at some threshold, what have you learned about human behavior? Use mostly English words, not numbers or stats, to explain your finding.)\n\ntreat = df[df['treatment'] == 1]['gave']\ncontrol = df[df['treatment'] == 0]['gave']\n\nmean_diff = treat.mean() - control.mean()\nvar_treat = treat.var()\nvar_control = control.var()\nn_treat = len(treat)\nn_control = len(control)\n\nt_stat = mean_diff / np.sqrt(var_treat / n_treat + var_control / n_control)\nprint(\"T-statistic:\", t_stat)\n\nT-statistic: 3.2094621908279835\n\n\n\nmodel = rsm.model.regress(data = df, rvar = 'gave', evar = 'treatment')\nmodel.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : gave\nExplanatory variables: treatment\nNull hyp.: the effect of x on gave is zero\nAlt. hyp.: the effect of x on gave is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.018      0.001   16.225  &lt; .001  ***\ntreatment        0.004      0.001    3.101   0.002   **\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: 0.0\nF-statistic: 9.618 df(1, 50081), p.value 0.002\nNr obs: 50,083\n\n\nThe statistical analysis shows that individuals who received the treatment — a matching donation offer — were significantly more likely to make a charitable contribution compared to those who received the control letter. Both the t-test and linear regression indicate that this difference is unlikely to be due to chance. This suggests that the presence of a matching grant acts as a powerful motivator, nudging people toward taking action. In the context of human behavior, this reveals that even a subtle change in framing — like knowing one’s gift will be matched — can meaningfully increase the likelihood of giving. It highlights how social cues or perceived amplification of impact can influence decision-making in charitable contexts.\ntodo: run a probit regression where the outcome variable is whether any charitable donation was made and the explanatory variable is assignment to treatment or control. Confirm that your results replicate Table 3 column 1 in the paper.\n\nfrom statsmodels.discrete.discrete_model import Probit\n\nprobit_model = Probit.from_formula('gave ~ treatment', data=df)\nprobit_result = probit_model.fit()\nprint(probit_result.summary())\nmfx = probit_result.get_margeff()\nprint(mfx.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                   gave   No. Observations:                50083\nModel:                         Probit   Df Residuals:                    50081\nMethod:                           MLE   Df Model:                            1\nDate:                Sun, 20 Apr 2025   Pseudo R-squ.:               0.0009783\nTime:                        15:47:35   Log-Likelihood:                -5030.5\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                  0.001696\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -2.1001      0.023    -90.073      0.000      -2.146      -2.054\ntreatment      0.0868      0.028      3.113      0.002       0.032       0.141\n==============================================================================\n       Probit Marginal Effects       \n=====================================\nDep. Variable:                   gave\nMethod:                          dydx\nAt:                           overall\n==============================================================================\n                dy/dx    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\ntreatment      0.0043      0.001      3.104      0.002       0.002       0.007\n==============================================================================\n\n\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\ntodo: Use a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not. For example, does the 2:1 match rate lead increase the likelihood that someone donates as compared to the 1:1 match rate? Do your results support the “figures suggest” comment the authors make on page 8?\n\n# t-test between 2:1 and 1:1 ratios\ngave_1_1 = df[df['ratio'] == 1]['gave']\ngave_2_1 = df[df['ratio'] == 2]['gave']\n\nfrom scipy.stats import ttest_ind\nt_stat, p_val = ttest_ind(gave_2_1, gave_1_1, equal_var=False)\n\nprint(\"2:1 vs 1:1 Match Rate:\")\nprint(f\"T-stat: {t_stat:.3f}, P-value: {p_val:.3f}\")\n\n2:1 vs 1:1 Match Rate:\nT-stat: 0.965, P-value: 0.335\n\n\n\n# t-test between 3:1 and 1:1 ratios\ngave_3_1 = df[df['ratio'] == 3]['gave']\ngave_1_1 = df[df['ratio'] == 1]['gave']\n\nt_stat, p_val = ttest_ind(gave_3_1, gave_1_1, equal_var=False)\nprint(\"3:1 vs 1:1 Match Rate:\")\nprint(f\"T-stat: {t_stat:.3f}, P-value: {p_val:.3f}\")\n\n3:1 vs 1:1 Match Rate:\nT-stat: 1.015, P-value: 0.310\n\n\n\n# t-test between 3:1 and 2:1 ratios\ngave_3_1 = df[df['ratio'] == 3]['gave']\ngave_2_1 = df[df['ratio'] == 2]['gave']\n\nt_stat, p_val = ttest_ind(gave_3_1, gave_2_1, equal_var=False)\nprint(\"3:1 vs 2:1 Match Rate:\")\nprint(f\"T-stat: {t_stat:.3f}, P-value: {p_val:.3f}\")\n\n3:1 vs 2:1 Match Rate:\nT-stat: 0.050, P-value: 0.960\n\n\nMy results clearly support the authors’ conclusion: while higher match ratios may show slightly higher donation rates numerically, those differences are not statistically significant, and therefore do not provide strong evidence that larger match sizes are more effective than 1:1 matches.\ntodo: Assess the same issue using a regression. Specifically, create the variable ratio1 then regress gave on ratio1, ratio2, and ratio3 (or alternatively, regress gave on the categorical variable ratio). Interpret the coefficients and their statistical precision.\n\nmodel = rsm.model.regress(data = df, rvar = 'gave', evar = 'ratio')\nmodel.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : gave\nExplanatory variables: ratio\nNull hyp.: the effect of x on gave is zero\nAlt. hyp.: the effect of x on gave is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.018      0.001   16.225  &lt; .001  ***\nratio[1]         0.003      0.002    1.661   0.097    .\nratio[2]         0.005      0.002    2.744   0.006   **\nratio[3]         0.005      0.002    2.802   0.005   **\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: 0.0\nF-statistic: 3.665 df(3, 50079), p.value 0.012\nNr obs: 50,083\n\n\nRegression results show that all three match ratios (1:1, 2:1, 3:1) are associated with slightly higher donation rates compared to the control group, but the differences are small. The coefficient for ratio[1] (1:1 match) is 0.003 and marginally significant (p = 0.097), while the coefficients for ratio[2] (2:1) and ratio[3] (3:1) are both 0.005 and statistically significant at the 1% level (p = 0.006 and p = 0.005). However, the differences between them are not large in magnitude — all are within 0.002 of each other — suggesting that although offering any match tends to increase donations, there is little evidence that larger match sizes lead to proportionally greater increases. This supports the conclusion that the presence of a match matters more than its generosity, and that people are generally responsive to the signal of support, not necessarily the size of the match.\ntodo: Calculate the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios. Do this directly from the data, and do it by computing the differences in the fitted coefficients of the previous regression. what do you conclude regarding the effectiveness of different sizes of matched donations?\n\np_1_1 = df[df['ratio'] == 1]['gave'].mean()\np_2_1 = df[df['ratio'] == 2]['gave'].mean()\np_3_1 = df[df['ratio'] == 3]['gave'].mean()\n\nprint(\"Response rates:\")\nprint(f\"1:1 = {p_1_1*100:.3f}%, 2:1 = {p_2_1*100:.3f}%, 3:1 = {p_3_1*100:.3f}%\")\n\nResponse rates:\n1:1 = 2.075%, 2:1 = 2.263%, 3:1 = 2.273%\n\n\nDifferences (From Regression Coefficients):\n2:1 − 1:1 = 0.005 − 0.003 = 0.002\n3:1 − 2:1 = 0.005 − 0.005 = 0.000\nBoth the raw response rates and regression coefficients indicate that increasing the match from 1:1 to 2:1 produces a small increase in the probability of donating (about 0.2 percentage points), while moving from 2:1 to 3:1 shows virtually no change. These differences are very modest in size, and the lack of improvement from 2:1 to 3:1 suggests diminishing returns to increasing match generosity. The findings reinforce the conclusion that offering any match increases donations, but offering a larger match ratio does not lead to proportionally greater giving. The signal of a match itself may be more powerful than the actual amount matched.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\ntodo: Calculate a t-test or run a bivariate linear regression of the donation amount on the treatment status. What do we learn from doing this analysis?\n\nmodel = rsm.model.regress(data = df, rvar = 'amount', evar = 'treatment')\nmodel.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : amount\nExplanatory variables: treatment\nNull hyp.: the effect of x on amount is zero\nAlt. hyp.: the effect of x on amount is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.813      0.067   12.063  &lt; .001  ***\ntreatment        0.154      0.083    1.861   0.063    .\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: 0.0\nF-statistic: 3.461 df(1, 50081), p.value 0.063\nNr obs: 50,083\n\n\nFrom this analysis, we learn that individuals who received the matching grant treatment donated slightly more on average than those in the control group, but the difference is only marginally statistically significant (p = 0.063). The treatment effect estimate suggests that the treatment group gave about 15 cents more per person than the control group, which is a small increase. This result implies that the primary effect of the matching grant is likely driven by increasing the number of people who give, rather than substantially increasing the donation amounts of those who would already have donated. It highlights that while the match offer motivates more people to donate, it doesn’t strongly influence how much they give on average.\ntodo: now limit the data to just people who made a donation and repeat the previous analysis. This regression allows you to analyze how much respondents donate conditional on donating some positive amount. Interpret the regression coefficients – what did we learn? Does the treatment coefficient have a causal interpretation?\n\nmodel = rsm.model.regress(data = df[df['gave'] == 1], rvar = 'amount', evar = 'treatment')\nmodel.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : amount\nExplanatory variables: treatment\nNull hyp.: the effect of x on amount is zero\nAlt. hyp.: the effect of x on amount is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept       45.540      2.423   18.792  &lt; .001  ***\ntreatment       -1.668      2.872   -0.581   0.561     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.001\nF-statistic: 0.337 df(1, 1032), p.value 0.561\nNr obs: 1,034\n\n\nThe regression results show that among individuals who made a donation, the treatment group gave slightly less on average than the control group, but this difference (−1.67) is not statistically significant (p = 0.561). This tells us that the matching grant treatment did not affect the amount given by those who chose to donate. In other words, the presence of a match increased the number of people who gave, but not how much they gave once they decided to give.\nAs for causal interpretation: since this regression is limited only to people who donated, it suffers from selection bias — treatment may have influenced who donated, and the group of donors in treatment and control may differ in unobserved ways. Therefore, the treatment coefficient cannot be interpreted causally in this conditional regression. Only the earlier regression using the full sample (which preserves random assignment) supports a causal interpretation of treatment effects.\ntodo: Make two plot: one for the treatment group and one for the control. Each plot should be a histogram of the donation amounts only among people who donated. Add a red vertical bar or some other annotation to indicate the sample average for each plot.\n\ndonors = df[df['gave'] == 1]\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\nsns.histplot(donors[donors['treatment'] == 0]['amount'], bins=30, ax=axes[0], color='skyblue')\naxes[0].axvline(donors[donors['treatment'] == 0]['amount'].mean(), color='red', linestyle='--')\naxes[0].set_title('Control Group')\naxes[0].set_xlabel('Donation Amount')\n\nsns.histplot(donors[donors['treatment'] == 1]['amount'], bins=30, ax=axes[1], color='lightgreen')\naxes[1].axvline(donors[donors['treatment'] == 1]['amount'].mean(), color='red', linestyle='--')\naxes[1].set_title('Treatment Group')\naxes[1].set_xlabel('Donation Amount')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "blog/project1/hw1_questions.html#simulation-experiment",
    "href": "blog/project1/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nto do: Make a plot like those on slide 43 from our first class and explain the plot to the reader. To do this, you will simulate 100,00 draws from the control distribution and 10,000 draws from the treatment distribution. You’ll then calculate a vector of 10,000 differences, and then you’ll plot the cumulative average of that vector of differences. Comment on whether the cumulative average approaches the true difference in means.\n\nn = 10000\np_control = 0.018\np_treatment = 0.022\ntrue_diff = p_treatment - p_control \n\ncontrol = np.random.binomial(1, p_control, n)\ntreatment = np.random.binomial(1, p_treatment, n)\n\ndiffs = treatment - control \n\ncumulative_avg = np.cumsum(diffs) / np.arange(1, n + 1)\n\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_avg, label='Cumulative Average Difference')\nplt.axhline(true_diff, color='red', linestyle='--', label='True Difference = 0.004')\nplt.title('Law of Large Numbers: Cumulative Average of Treatment - Control')\nplt.xlabel('Number of Simulated Pairs')\nplt.ylabel('Average Difference')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe cumulative average of the difference in donation outcomes between the treatment and control groups begins with high variability but quickly stabilizes as more simulated pairs are added. By around 1,000–2,000 iterations, the average difference converges very closely to the true population difference of 0.004, shown by the red dashed line. This confirms that as the sample size increases, the sample average becomes a reliable and consistent estimator of the population average. In other words, the plot clearly shows that the cumulative average approaches the true difference in means, which is exactly what the Law of Large Numbers predicts.\n\n\nCentral Limit Theorem\nto do: Make 4 histograms like those on slide 44 from our first class at sample sizes 50, 200, 500, and 1000 and explain these plots to the reader. To do this for a sample size of e.g. 50, take 50 draws from each of the control and treatment distributions, and calculate the average difference between those draws. Then repeat that process 999 more times so that you have 1000 averages. Plot the histogram of those averages. Comment on whether zero is in the “middle” of the distribution or whether it’s in the “tail.”\n\np_control = 0.018\np_treatment = 0.022\ntrue_diff = p_treatment - p_control\nn_sim = 1000\nsample_sizes = [50, 200, 500, 1000]\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\naxes = axes.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    avg_diffs = []\n\n    for _ in range(n_sim):\n        control_sample = np.random.binomial(1, p_control, n)\n        treatment_sample = np.random.binomial(1, p_treatment, n)\n        diff = treatment_sample.mean() - control_sample.mean()\n        avg_diffs.append(diff)\n    \n    sns.histplot(avg_diffs, bins=30, kde=True, stat=\"frequency\", color=\"blue\", ax=axes[i])\n    axes[i].axvline(0, color='red', linestyle='--', linewidth=2)\n    axes[i].set_title(f\"Sample size = {n}\", fontsize=14)\n    axes[i].set_xlabel(\"Mean Difference (Treatment - Control)\")\n    axes[i].set_ylabel(\"Frequency\")\n\nplt.suptitle(\"Central Limit Theorem Simulation\", fontsize=16, y=1.02)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nIn the histogram for a sample size of 50, the distribution is wide and irregular, and zero appears near the center — indicating that at this small sample size, it’s still quite common to observe no difference between treatment and control groups due to random variation. As the sample size increases to 200 and 500, the distributions become more symmetric and bell-shaped, with zero gradually shifting away from the peak. By the time we reach a sample size of 1000, the distribution is tightly centered around a positive value, and zero clearly lies in the tail of the distribution. This shift demonstrates how larger samples improve our ability to detect even small differences and reduce the likelihood of falsely concluding that there’s no effect when one actually exists. In short, as sample size increases, zero moves from the middle toward the tails, confirming the predictions of the Central Limit Theorem and the increasing statistical power of larger samples."
  },
  {
    "objectID": "blog/project1/index.html",
    "href": "blog/project1/index.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/project1/index.html#introduction",
    "href": "blog/project1/index.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/project1/index.html#data",
    "href": "blog/project1/index.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\ntodo: Read the data into R/Python and describe the data\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pyrsm as rsm\ndf = pd.read_stata(r\"karlan_list_2007.dta\")\ndf.describe()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio2\nratio3\nsize25\nsize50\nsize100\nsizeno\naskd1\naskd2\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\ncount\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n...\n49978.000000\n49978.000000\n48217.000000\n48047.000000\n48217.000000\n48221.000000\n48209.000000\n48214.000000\n48215.000000\n48217.000000\n\n\nmean\n0.666813\n0.333187\n0.222311\n0.222211\n0.166723\n0.166623\n0.166723\n0.166743\n0.222311\n0.222291\n...\n0.510245\n0.488715\n0.819599\n0.086710\n0.321694\n2.429012\n54815.700533\n0.669418\n0.391661\n0.871968\n\n\nstd\n0.471357\n0.471357\n0.415803\n0.415736\n0.372732\n0.372643\n0.372732\n0.372750\n0.415803\n0.415790\n...\n0.499900\n0.499878\n0.168561\n0.135868\n0.103039\n0.378115\n22027.316665\n0.193405\n0.186599\n0.258654\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.009418\n0.000000\n0.000000\n0.000000\n5000.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.755845\n0.014729\n0.258311\n2.210000\n39181.000000\n0.560222\n0.235647\n0.884929\n\n\n50%\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n0.000000\n0.872797\n0.036554\n0.305534\n2.440000\n50673.000000\n0.712296\n0.373744\n1.000000\n\n\n75%\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n1.000000\n0.938827\n0.090882\n0.369132\n2.660000\n66005.000000\n0.816798\n0.530036\n1.000000\n\n\nmax\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n...\n1.000000\n1.000000\n1.000000\n0.989622\n0.997544\n5.270000\n200001.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n8 rows × 48 columns\n\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\ntodo: test a few variables other than the key outcome variables (for example, test months since last donation) to see if the treatment and control groups are statistically significantly different at the 95% confidence level. Do each as a t-test and separately as a linear regression, and confirm you get the exact same results from both methods. When doing a t-test, use the formula in the class slides. When doing the linear regression, regress for example mrm2 on treatment and look at the estimated coefficient on the treatment variable. It might be helpful to compare parts of your analysis to Table 1 in the paper. Be sure to comment on your results (hint: why is Table 1 included in the paper).\n\n\n“mrm2”\n\ntreat = df[df['treatment'] == 1]['mrm2']\ncontrol = df[df['treatment'] == 0]['mrm2']\n\nmean_treat = treat.mean()\nmean_control = control.mean()\nvar_treat = treat.var(ddof=1)\nvar_control = control.var(ddof=1)\nn_treat = treat.count()\nn_control = control.count()\n\nnumerator = mean_treat - mean_control\ndenominator = np.sqrt(var_treat / n_treat + var_control / n_control)\nt_stat = numerator / denominator\n\nprint(\"Mean Treatment:\", round(mean_treat,3))\nprint(\"Std Treatment:\", round(np.sqrt(var_treat),3))\n\nprint(\"Mean Control:\", round(mean_control,3))\nprint(\"Std Control:\", round(np.sqrt(var_control),3))\nprint(\"Overall Mean:\", round(df['mrm2'].mean(),3))\nprint(\"Overall Std:\", round(df['mrm2'].std(),3))\nprint(\"t test:\", t_stat)\n\nMean Treatment: 13.012\nStd Treatment: 12.086\nMean Control: 12.998\nStd Control: 12.074\nOverall Mean: 13.007\nOverall Std: 12.081\nt test: 0.1195315522817725\n\n\n\nmodel = rsm.model.regress(data = df, rvar = 'mrm2', evar = 'treatment')\nmodel.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : mrm2\nExplanatory variables: treatment\nNull hyp.: the effect of x on mrm2 is zero\nAlt. hyp.: the effect of x on mrm2 is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept       12.998      0.094  138.979  &lt; .001  ***\ntreatment        0.014      0.115    0.119   0.905     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.0\nF-statistic: 0.014 df(1, 50080), p.value 0.905\nNr obs: 50,082 (1 obs. dropped)\n\n\nThe manual t-test and linear regression results for mrm2 (months since last donation) show no statistically significant difference between the treatment and control groups (t ≈ 0.12, p ≈ 0.905), indicating that this key pre-treatment variable is well balanced. This supports the validity of the random assignment and confirms that the experimental groups are comparable before the intervention. Table 1 is included in the paper to show that randomization produced balanced groups across a wide range of covariates, ensuring that any observed differences in outcomes can be attributed to the treatment rather than pre-existing differences.\n\n\n“hpa”\n\ntreat = df[df['treatment'] == 1]['hpa']\ncontrol = df[df['treatment'] == 0]['hpa']\n\nmean_treat = treat.mean()\nmean_control = control.mean()\nvar_treat = treat.var(ddof=1)\nvar_control = control.var(ddof=1)\nn_treat = treat.count()\nn_control = control.count()\n\nnumerator = mean_treat - mean_control\ndenominator = np.sqrt(var_treat / n_treat + var_control / n_control)\nt_stat = numerator / denominator\n\nprint(\"Mean Treatment:\", round(mean_treat,3))\nprint(\"Std Treatment:\", round(np.sqrt(var_treat),3))\n\nprint(\"Mean Control:\", round(mean_control,3))\nprint(\"Std Control:\", round(np.sqrt(var_control),3))\nprint(\"Overall Mean:\", round(df['hpa'].mean(),3))\nprint(\"Overall Std:\", round(df['hpa'].std(),3))\nprint(\"t test:\", t_stat)\n\nMean Treatment: 59.597\nStd Treatment: 73.052\nMean Control: 58.96\nStd Control: 67.269\nOverall Mean: 59.385\nOverall Std: 71.177\nt test: 0.9704273516144837\n\n\n\nmodel = rsm.model.regress(data = df, rvar = 'hpa', evar = 'treatment')\nmodel.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : hpa\nExplanatory variables: treatment\nNull hyp.: the effect of x on hpa is zero\nAlt. hyp.: the effect of x on hpa is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept       58.960      0.551  107.005  &lt; .001  ***\ntreatment        0.637      0.675    0.944   0.345     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.0\nF-statistic: 0.891 df(1, 50081), p.value 0.345\nNr obs: 50,083\n\n\nThe hpa variable is a pre-treatment covariate, so the lack of a significant difference across groups provides evidence that random assignment was successful. In particular, it suggests that prior donation behavior (as measured by the highest past contribution) was balanced across the treatment and control conditions. This supports the validity of any causal claims made later in the experiment, as we can reasonably assume that any post-treatment differences in donation behavior are due to the treatment itself rather than pre-existing differences in donor generosity.\n\n\n“freq”\n\ntreat = df[df['treatment'] == 1]['freq']\ncontrol = df[df['treatment'] == 0]['freq']\n\nmean_treat = treat.mean()\nmean_control = control.mean()\nvar_treat = treat.var(ddof=1)\nvar_control = control.var(ddof=1)\nn_treat = treat.count()\nn_control = control.count()\n\nnumerator = mean_treat - mean_control\ndenominator = np.sqrt(var_treat / n_treat + var_control / n_control)\nt_stat = numerator / denominator\n\nprint(\"Mean Treatment:\", round(mean_treat,3))\nprint(\"Std Treatment:\", round(np.sqrt(var_treat),3))\n\nprint(\"Mean Control:\", round(mean_control,3))\nprint(\"Std Control:\", round(np.sqrt(var_control),3))\nprint(\"Overall Mean:\", round(df['freq'].mean(),3))\nprint(\"Overall Std:\", round(df['freq'].std(),3))\nprint(\"t test:\", t_stat)\n\nMean Treatment: 8.035\nStd Treatment: 11.39\nMean Control: 8.047\nStd Control: 11.404\nOverall Mean: 8.039\nOverall Std: 11.394\nt test: -0.11084502380904246\n\n\n\nmodel = rsm.model.regress(data = df, rvar = 'freq', evar = 'treatment')\nmodel.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : freq\nExplanatory variables: treatment\nNull hyp.: the effect of x on freq is zero\nAlt. hyp.: the effect of x on freq is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        8.047      0.088   91.231  &lt; .001  ***\ntreatment       -0.012      0.108   -0.111   0.912     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.0\nF-statistic: 0.012 df(1, 50081), p.value 0.912\nNr obs: 50,083\n\n\nThe variable freq, representing the number of prior donations, shows nearly identical means between the treatment and control groups. Both the manual t-test and regression confirm that this difference is not statistically significant. This suggests that random assignment successfully balanced this pre-treatment characteristic, which supports the internal validity of the experimental design. Because the treatment and control groups are statistically equivalent in terms of donation frequency history, we can be more confident that any observed post-treatment effects on donation behavior are attributable to the treatment rather than pre-existing differences in donor activity."
  },
  {
    "objectID": "blog/project1/index.html#experimental-results",
    "href": "blog/project1/index.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\ntodo: make a barplot with two bars. Each bar is the proportion of people who donated. One bar for treatment and one bar for control.\n\nprop_donated = df.groupby('treatment')['gave'].mean().reset_index()\nprop_donated['group'] = prop_donated['treatment'].map({0: 'Control', 1: 'Treatment'})\n\nsns.barplot(data=prop_donated, x='group', y='gave')\nplt.ylabel(\"Proportion Who Donated\")\nplt.title(\"Donation Rate by Group\")\nplt.show()\n\n\n\n\n\n\n\n\ntodo: run a t-test between the treatment and control groups on the binary outcome of whether any charitable donation was made. Also run a bivariate linear regression that demonstrates the same finding. (It may help to confirm your calculations match Table 2a Panel A.) Report your statistical results and interpret them in the context of the experiment (e.g., if you found a difference with a small p-value or that was statistically significant at some threshold, what have you learned about human behavior? Use mostly English words, not numbers or stats, to explain your finding.)\n\ntreat = df[df['treatment'] == 1]['gave']\ncontrol = df[df['treatment'] == 0]['gave']\n\nmean_diff = treat.mean() - control.mean()\nvar_treat = treat.var()\nvar_control = control.var()\nn_treat = len(treat)\nn_control = len(control)\n\nt_stat = mean_diff / np.sqrt(var_treat / n_treat + var_control / n_control)\nprint(\"T-statistic:\", t_stat)\n\nT-statistic: 3.2094621908279835\n\n\n\nmodel = rsm.model.regress(data = df, rvar = 'gave', evar = 'treatment')\nmodel.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : gave\nExplanatory variables: treatment\nNull hyp.: the effect of x on gave is zero\nAlt. hyp.: the effect of x on gave is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.018      0.001   16.225  &lt; .001  ***\ntreatment        0.004      0.001    3.101   0.002   **\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: 0.0\nF-statistic: 9.618 df(1, 50081), p.value 0.002\nNr obs: 50,083\n\n\nThe statistical analysis shows that individuals who received the treatment — a matching donation offer — were significantly more likely to make a charitable contribution compared to those who received the control letter. Both the t-test and linear regression indicate that this difference is unlikely to be due to chance. This suggests that the presence of a matching grant acts as a powerful motivator, nudging people toward taking action. In the context of human behavior, this reveals that even a subtle change in framing — like knowing one’s gift will be matched — can meaningfully increase the likelihood of giving. It highlights how social cues or perceived amplification of impact can influence decision-making in charitable contexts.\ntodo: run a probit regression where the outcome variable is whether any charitable donation was made and the explanatory variable is assignment to treatment or control. Confirm that your results replicate Table 3 column 1 in the paper.\n\nfrom statsmodels.discrete.discrete_model import Probit\n\nprobit_model = Probit.from_formula('gave ~ treatment', data=df)\nprobit_result = probit_model.fit()\nprint(probit_result.summary())\nmfx = probit_result.get_margeff()\nprint(mfx.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                   gave   No. Observations:                50083\nModel:                         Probit   Df Residuals:                    50081\nMethod:                           MLE   Df Model:                            1\nDate:                Wed, 23 Apr 2025   Pseudo R-squ.:               0.0009783\nTime:                        13:40:29   Log-Likelihood:                -5030.5\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                  0.001696\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -2.1001      0.023    -90.073      0.000      -2.146      -2.054\ntreatment      0.0868      0.028      3.113      0.002       0.032       0.141\n==============================================================================\n       Probit Marginal Effects       \n=====================================\nDep. Variable:                   gave\nMethod:                          dydx\nAt:                           overall\n==============================================================================\n                dy/dx    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\ntreatment      0.0043      0.001      3.104      0.002       0.002       0.007\n==============================================================================\n\n\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\ntodo: Use a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not. For example, does the 2:1 match rate lead increase the likelihood that someone donates as compared to the 1:1 match rate? Do your results support the “figures suggest” comment the authors make on page 8?\n\n# t-test between 2:1 and 1:1 ratios\ngave_1_1 = df[df['ratio'] == 1]['gave']\ngave_2_1 = df[df['ratio'] == 2]['gave']\n\nfrom scipy.stats import ttest_ind\nt_stat, p_val = ttest_ind(gave_2_1, gave_1_1, equal_var=False)\n\nprint(\"2:1 vs 1:1 Match Rate:\")\nprint(f\"T-stat: {t_stat:.3f}, P-value: {p_val:.3f}\")\n\n2:1 vs 1:1 Match Rate:\nT-stat: 0.965, P-value: 0.335\n\n\n\n# t-test between 3:1 and 1:1 ratios\ngave_3_1 = df[df['ratio'] == 3]['gave']\ngave_1_1 = df[df['ratio'] == 1]['gave']\n\nt_stat, p_val = ttest_ind(gave_3_1, gave_1_1, equal_var=False)\nprint(\"3:1 vs 1:1 Match Rate:\")\nprint(f\"T-stat: {t_stat:.3f}, P-value: {p_val:.3f}\")\n\n3:1 vs 1:1 Match Rate:\nT-stat: 1.015, P-value: 0.310\n\n\n\n# t-test between 3:1 and 2:1 ratios\ngave_3_1 = df[df['ratio'] == 3]['gave']\ngave_2_1 = df[df['ratio'] == 2]['gave']\n\nt_stat, p_val = ttest_ind(gave_3_1, gave_2_1, equal_var=False)\nprint(\"3:1 vs 2:1 Match Rate:\")\nprint(f\"T-stat: {t_stat:.3f}, P-value: {p_val:.3f}\")\n\n3:1 vs 2:1 Match Rate:\nT-stat: 0.050, P-value: 0.960\n\n\nMy results clearly support the authors’ conclusion: while higher match ratios may show slightly higher donation rates numerically, those differences are not statistically significant, and therefore do not provide strong evidence that larger match sizes are more effective than 1:1 matches.\ntodo: Assess the same issue using a regression. Specifically, create the variable ratio1 then regress gave on ratio1, ratio2, and ratio3 (or alternatively, regress gave on the categorical variable ratio). Interpret the coefficients and their statistical precision.\n\nmodel = rsm.model.regress(data = df, rvar = 'gave', evar = 'ratio')\nmodel.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : gave\nExplanatory variables: ratio\nNull hyp.: the effect of x on gave is zero\nAlt. hyp.: the effect of x on gave is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.018      0.001   16.225  &lt; .001  ***\nratio[1]         0.003      0.002    1.661   0.097    .\nratio[2]         0.005      0.002    2.744   0.006   **\nratio[3]         0.005      0.002    2.802   0.005   **\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: 0.0\nF-statistic: 3.665 df(3, 50079), p.value 0.012\nNr obs: 50,083\n\n\nRegression results show that all three match ratios (1:1, 2:1, 3:1) are associated with slightly higher donation rates compared to the control group, but the differences are small. The coefficient for ratio[1] (1:1 match) is 0.003 and marginally significant (p = 0.097), while the coefficients for ratio[2] (2:1) and ratio[3] (3:1) are both 0.005 and statistically significant at the 1% level (p = 0.006 and p = 0.005). However, the differences between them are not large in magnitude — all are within 0.002 of each other — suggesting that although offering any match tends to increase donations, there is little evidence that larger match sizes lead to proportionally greater increases. This supports the conclusion that the presence of a match matters more than its generosity, and that people are generally responsive to the signal of support, not necessarily the size of the match.\ntodo: Calculate the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios. Do this directly from the data, and do it by computing the differences in the fitted coefficients of the previous regression. what do you conclude regarding the effectiveness of different sizes of matched donations?\n\np_1_1 = df[df['ratio'] == 1]['gave'].mean()\np_2_1 = df[df['ratio'] == 2]['gave'].mean()\np_3_1 = df[df['ratio'] == 3]['gave'].mean()\n\nprint(\"Response rates:\")\nprint(f\"1:1 = {p_1_1*100:.3f}%, 2:1 = {p_2_1*100:.3f}%, 3:1 = {p_3_1*100:.3f}%\")\n\nResponse rates:\n1:1 = 2.075%, 2:1 = 2.263%, 3:1 = 2.273%\n\n\nDifferences (From Regression Coefficients):\n2:1 − 1:1 = 0.005 − 0.003 = 0.002\n3:1 − 2:1 = 0.005 − 0.005 = 0.000\nBoth the raw response rates and regression coefficients indicate that increasing the match from 1:1 to 2:1 produces a small increase in the probability of donating (about 0.2 percentage points), while moving from 2:1 to 3:1 shows virtually no change. These differences are very modest in size, and the lack of improvement from 2:1 to 3:1 suggests diminishing returns to increasing match generosity. The findings reinforce the conclusion that offering any match increases donations, but offering a larger match ratio does not lead to proportionally greater giving. The signal of a match itself may be more powerful than the actual amount matched.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\ntodo: Calculate a t-test or run a bivariate linear regression of the donation amount on the treatment status. What do we learn from doing this analysis?\n\nmodel = rsm.model.regress(data = df, rvar = 'amount', evar = 'treatment')\nmodel.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : amount\nExplanatory variables: treatment\nNull hyp.: the effect of x on amount is zero\nAlt. hyp.: the effect of x on amount is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.813      0.067   12.063  &lt; .001  ***\ntreatment        0.154      0.083    1.861   0.063    .\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: 0.0\nF-statistic: 3.461 df(1, 50081), p.value 0.063\nNr obs: 50,083\n\n\nFrom this analysis, we learn that individuals who received the matching grant treatment donated slightly more on average than those in the control group, but the difference is only marginally statistically significant (p = 0.063). The treatment effect estimate suggests that the treatment group gave about 15 cents more per person than the control group, which is a small increase. This result implies that the primary effect of the matching grant is likely driven by increasing the number of people who give, rather than substantially increasing the donation amounts of those who would already have donated. It highlights that while the match offer motivates more people to donate, it doesn’t strongly influence how much they give on average.\ntodo: now limit the data to just people who made a donation and repeat the previous analysis. This regression allows you to analyze how much respondents donate conditional on donating some positive amount. Interpret the regression coefficients – what did we learn? Does the treatment coefficient have a causal interpretation?\n\nmodel = rsm.model.regress(data = df[df['gave'] == 1], rvar = 'amount', evar = 'treatment')\nmodel.summary()\n\nLinear regression (OLS)\nData                 : Not provided\nResponse variable    : amount\nExplanatory variables: treatment\nNull hyp.: the effect of x on amount is zero\nAlt. hyp.: the effect of x on amount is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept       45.540      2.423   18.792  &lt; .001  ***\ntreatment       -1.668      2.872   -0.581   0.561     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.001\nF-statistic: 0.337 df(1, 1032), p.value 0.561\nNr obs: 1,034\n\n\nThe regression results show that among individuals who made a donation, the treatment group gave slightly less on average than the control group, but this difference (−1.67) is not statistically significant (p = 0.561). This tells us that the matching grant treatment did not affect the amount given by those who chose to donate. In other words, the presence of a match increased the number of people who gave, but not how much they gave once they decided to give.\nAs for causal interpretation: since this regression is limited only to people who donated, it suffers from selection bias — treatment may have influenced who donated, and the group of donors in treatment and control may differ in unobserved ways. Therefore, the treatment coefficient cannot be interpreted causally in this conditional regression. Only the earlier regression using the full sample (which preserves random assignment) supports a causal interpretation of treatment effects.\ntodo: Make two plot: one for the treatment group and one for the control. Each plot should be a histogram of the donation amounts only among people who donated. Add a red vertical bar or some other annotation to indicate the sample average for each plot.\n\ndonors = df[df['gave'] == 1]\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\nsns.histplot(donors[donors['treatment'] == 0]['amount'], bins=30, ax=axes[0], color='skyblue')\naxes[0].axvline(donors[donors['treatment'] == 0]['amount'].mean(), color='red', linestyle='--')\naxes[0].set_title('Control Group')\naxes[0].set_xlabel('Donation Amount')\n\nsns.histplot(donors[donors['treatment'] == 1]['amount'], bins=30, ax=axes[1], color='lightgreen')\naxes[1].axvline(donors[donors['treatment'] == 1]['amount'].mean(), color='red', linestyle='--')\naxes[1].set_title('Treatment Group')\naxes[1].set_xlabel('Donation Amount')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "blog/project1/index.html#simulation-experiment",
    "href": "blog/project1/index.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nto do: Make a plot like those on slide 43 from our first class and explain the plot to the reader. To do this, you will simulate 100,00 draws from the control distribution and 10,000 draws from the treatment distribution. You’ll then calculate a vector of 10,000 differences, and then you’ll plot the cumulative average of that vector of differences. Comment on whether the cumulative average approaches the true difference in means.\n\nn = 10000\np_control = 0.018\np_treatment = 0.022\ntrue_diff = p_treatment - p_control \n\ncontrol = np.random.binomial(1, p_control, n)\ntreatment = np.random.binomial(1, p_treatment, n)\n\ndiffs = treatment - control \n\ncumulative_avg = np.cumsum(diffs) / np.arange(1, n + 1)\n\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_avg, label='Cumulative Average Difference')\nplt.axhline(true_diff, color='red', linestyle='--', label='True Difference = 0.004')\nplt.title('Law of Large Numbers: Cumulative Average of Treatment - Control')\nplt.xlabel('Number of Simulated Pairs')\nplt.ylabel('Average Difference')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe cumulative average of the difference in donation outcomes between the treatment and control groups begins with high variability but quickly stabilizes as more simulated pairs are added. By around 1,000–2,000 iterations, the average difference converges very closely to the true population difference of 0.004, shown by the red dashed line. This confirms that as the sample size increases, the sample average becomes a reliable and consistent estimator of the population average. In other words, the plot clearly shows that the cumulative average approaches the true difference in means, which is exactly what the Law of Large Numbers predicts.\n\n\nCentral Limit Theorem\nto do: Make 4 histograms like those on slide 44 from our first class at sample sizes 50, 200, 500, and 1000 and explain these plots to the reader. To do this for a sample size of e.g. 50, take 50 draws from each of the control and treatment distributions, and calculate the average difference between those draws. Then repeat that process 999 more times so that you have 1000 averages. Plot the histogram of those averages. Comment on whether zero is in the “middle” of the distribution or whether it’s in the “tail.”\n\np_control = 0.018\np_treatment = 0.022\ntrue_diff = p_treatment - p_control\nn_sim = 1000\nsample_sizes = [50, 200, 500, 1000]\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\naxes = axes.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    avg_diffs = []\n\n    for _ in range(n_sim):\n        control_sample = np.random.binomial(1, p_control, n)\n        treatment_sample = np.random.binomial(1, p_treatment, n)\n        diff = treatment_sample.mean() - control_sample.mean()\n        avg_diffs.append(diff)\n    \n    sns.histplot(avg_diffs, bins=30, kde=True, stat=\"frequency\", color=\"blue\", ax=axes[i])\n    axes[i].axvline(0, color='red', linestyle='--', linewidth=2)\n    axes[i].set_title(f\"Sample size = {n}\", fontsize=14)\n    axes[i].set_xlabel(\"Mean Difference (Treatment - Control)\")\n    axes[i].set_ylabel(\"Frequency\")\n\nplt.suptitle(\"Central Limit Theorem Simulation\", fontsize=16, y=1.02)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nIn the histogram for a sample size of 50, the distribution is wide and irregular, and zero appears near the center — indicating that at this small sample size, it’s still quite common to observe no difference between treatment and control groups due to random variation. As the sample size increases to 200 and 500, the distributions become more symmetric and bell-shaped, with zero gradually shifting away from the peak. By the time we reach a sample size of 1000, the distribution is tightly centered around a positive value, and zero clearly lies in the tail of the distribution. This shift demonstrates how larger samples improve our ability to detect even small differences and reduce the likelihood of falsely concluding that there’s no effect when one actually exists. In short, as sample size increases, zero moves from the middle toward the tails, confirming the predictions of the Central Limit Theorem and the increasing statistical power of larger samples."
  }
]